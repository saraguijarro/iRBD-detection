{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE READING & PREPROCESSING\n",
    "# Transform raw accelerometer files into clean night-segmented data suitable for feature extraction.\n",
    "\n",
    "## INPUT\n",
    "# Source : .CWA and .cwa files from Axivity accelerometer devices. \n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/data/raw/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/raw/irbd/ \n",
    "# Format : Binary accelerometer data files containing continuous recordings\n",
    "\n",
    "\n",
    "## PIPELINE\n",
    "# 1. File reading : Use actipy.read_device() with 12Hz lowpass and 30Hz resampling\n",
    "# 2. Night segmentation : Extract 8-hour periods from 22:00 to 06:00\n",
    "# 3. Quality control : \n",
    "#    - Apply 17 degrees Celcius temperature threshold fitering\n",
    "#    - Remove nights flagged as non-wear by actipy\n",
    "#    - Validate data integrity and sampling rate consistency\n",
    "# 4. Save preprocessed data : Create a .h5 file for each .cwa file, with the clean preprocessed data\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# Format : .h5 files (one per input .cwa file)\n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/data/preprocessed/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/preprocessed/irbd/ \n",
    "# Structure (for each .h5 file, consistent across all nights) :\n",
    "#   ├── name (attribute)             # Participant identifier\n",
    "#   ├── number_of_nights (attribute) # Total valid nights\n",
    "#   └── datasets/\n",
    "#       ├── night1/\n",
    "#       │ ├── x                      # X-axis accelerometer data (30Hz)\n",
    "#       │ ├── y                      # Y-axis accelerometer data (30Hz)\n",
    "#       │ ├── z                      # Z-axis accelerometer data (30Hz)\n",
    "#       │ └── timestamps             # Corresponding timestamps\n",
    "#       ├── night2/\n",
    "#       │ ├── x\n",
    "#       │ ├── y\n",
    "#       │ ├── z\n",
    "#       │ └── timestamps\n",
    "#       └── ...\n",
    "\n",
    "\n",
    "## VALIDATION\n",
    "# - Frequency analysis to confirm 12Hz filter preserves iRBD-relevant signals\n",
    "# - Sampling rate validation to ensure 30Hz captures movement dynamics  \n",
    "# - Structure validation to ensure consistency across all files\n",
    "# - Temperature threshold effectiveness for non-wear detection\n",
    "\n",
    "\n",
    "# Notes : Make sure to be able to read .cwa and .CWA files.\n",
    "\n",
    "\n",
    "## PARAMETERS SUMMARY\n",
    "# Library : actipy\n",
    "# Lowpass Filter : 12Hz (sleep movement studies typically use 10-15Hz lowpass filters)\n",
    "# Resampling : 30Hz (Nyquist theorem: 2 × highest frequency of interest, most sleep actigraphy studies use 25-50Hz)\n",
    "# Night Segmentation : 22:00-06:00 (8-hour sleep periods)\n",
    "# Temperature Threshold : 17 degrees Celcius (iRBD literature standard)\n",
    "# Non-wear Detection : Automatic flagging by actipy, then this script removes the flagged nights\n",
    "\n",
    "\n",
    "## ENVIRONMENT : env_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " actipy version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Basic Python libraries for file operations and system control\n",
    "import os                    # Operating System interface - helps us work with files and folders\n",
    "import sys                   # System-specific parameters - helps us control the program execution\n",
    "import h5py                  # HDF5 library - for saving the processed data in an efficient format\n",
    "import numpy as np           # NumPy - for mathematical operations on arrays of numbers\n",
    "import pandas as pd          # Pandas - for working with data tables and organizing information\n",
    "from datetime import datetime, timedelta, time  # For working with dates and times\n",
    "import logging               # For creating detailed log files that record what the program does\n",
    "from pathlib import Path     # For easier and more reliable file path handling\n",
    "import glob                  # For finding files that match specific patterns (like all .cwa files)\n",
    "import traceback             # For showing detailed error messages when something goes wrong\n",
    "\n",
    "# Visualization libraries for creating plots and charts\n",
    "import matplotlib.pyplot as plt    # Main plotting library - like creating graphs in Excel\n",
    "import seaborn as sns             # Statistical plotting library - makes beautiful, professional plots\n",
    "\n",
    "# Configure matplotlib and seaborn for professional-looking plots\n",
    "plt.style.use('seaborn-v0_8')     # Use seaborn's visual style (makes plots look professional)\n",
    "sns.set_palette(\"husl\")           # Set a nice color palette (colors that work well together)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)  # Set default size for all plots (12 inches wide, 8 inches tall)\n",
    "plt.rcParams['font.size'] = 10    # Set default font size for all text in plots\n",
    "\n",
    "# Try to import actipy - this is the main library for reading accelerometer data\n",
    "try:\n",
    "    import actipy              # actipy - specialized library for processing accelerometer data\n",
    "    print(f\"actipy version: {actipy.__version__}\")\n",
    "except ImportError as e:\n",
    "    # If actipy is not installed, show an error message and stop the program\n",
    "    print(f\"Error importing actipy: {e}\")\n",
    "    print(\"Please install actipy with: pip install actipy\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPROCESSING PIPELINE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    This class handles the complete preprocessing of accelerometer data for iRBD detection.\n",
    "    \n",
    "    WHAT iRBD IS:\n",
    "    iRBD (idiopathic REM sleep Behavior Disorder) is a sleep disorder where people\n",
    "    act out their dreams. We can detect it by analyzing movement patterns during sleep.\n",
    "    \n",
    "    WHAT THIS CLASS DOES:\n",
    "    1. Reads raw .cwa files from Axivity accelerometer devices\n",
    "    2. Applies signal processing (12Hz lowpass filter, 30Hz resampling)\n",
    "    3. Segments data into night periods (22:00-06:00)\n",
    "    4. Removes poor quality data (temperature filtering, non-wear detection)\n",
    "    5. Saves clean data in HDF5 format for the next pipeline stage\n",
    "    \n",
    "    WHY THESE PARAMETERS:\n",
    "    - 12Hz lowpass: Sleep movements are slow, we don't need high frequencies\n",
    "    - 30Hz resampling: Captures all relevant movement while keeping file sizes manageable\n",
    "    - 22:00-06:00: Standard 8-hour sleep period used in sleep research\n",
    "    - 17°C threshold: When worn, body heat warms the device above 17°C\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessing pipeline with all necessary configuration.\n",
    "        This sets up directories, parameters, and logging systems.\n",
    "        \"\"\"\n",
    "        \n",
    "        # =================================================================\n",
    "        # CONFIGURATION SECTION - Choose between example test and full processing\n",
    "        # =================================================================\n",
    "        \n",
    "        # FOR EXAMPLE FILE TESTING\n",
    "        self.base_dir = Path(\"/work3/s184484/iRBD-detection\")  # Main project folder on HPC\n",
    "        # self.mode = \"EXAMPLE_TEST\"                             # Tell the script we're testing with example\n",
    "        self.example_participant = \"2290025_90001_0_0\"        # Example participant ID (without .cwa extension)\n",
    "        \n",
    "        # FOR FULL DATASET PROCESSING\n",
    "        self.mode = \"FULL_PROCESSING\"                          # Tell the script we're processing all files\n",
    "        \n",
    "        # =================================================================\n",
    "        # DIRECTORY SETUP - Define where to find files and save results\n",
    "        # =================================================================\n",
    "        \n",
    "        # Input directories (where the raw .cwa files are stored):\n",
    "        self.raw_controls_dir = self.base_dir / \"data\" / \"raw\" / \"controls\"  # Healthy people's data\n",
    "        self.raw_irbd_dir = self.base_dir / \"data\" / \"raw\" / \"irbd\"          # iRBD patients' data\n",
    "        self.raw_example_file = self.base_dir / \"data\" / \"raw\" / f\"{self.example_participant}.cwa\"  # Example file path\n",
    "        \n",
    "        # Output directories (where to save the preprocessed .h5 files):\n",
    "        self.preprocessed_controls_dir = self.base_dir / \"data\" / \"preprocessed\" / \"controls\"  # Processed healthy data\n",
    "        self.preprocessed_irbd_dir = self.base_dir / \"data\" / \"preprocessed\" / \"irbd\"          # Processed iRBD data\n",
    "        \n",
    "        # Visualization output directory (where to save plots for the report):\n",
    "        self.plots_dir = self.base_dir / \"results\" / \"visualizations\"\n",
    "        self.example_plots_dir = self.plots_dir / \"example_testing\"  # Specific folder for example file plots\n",
    "        \n",
    "        # Only create log directory if it doesn't exist (for logging only)\n",
    "        self.log_dir = self.base_dir / \"validation\" / \"data_quality_reports\"\n",
    "        if not self.log_dir.exists():\n",
    "            self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # =================================================================\n",
    "        # PROCESSING PARAMETERS - Settings that control how data is processed\n",
    "        # =================================================================\n",
    "        \n",
    "        # actipy processing parameters (how we read and filter the raw data):\n",
    "        self.lowpass_hz = 12                    # Remove frequencies above 12Hz (sleep movements are slow)\n",
    "        self.resample_hz = 30                   # Resample to 30Hz (good balance of detail vs file size)\n",
    "        self.detect_nonwear = True              # Let actipy automatically detect when device wasn't worn\n",
    "        self.calibrate_gravity = True          # Correct for gravity and device orientation\n",
    "        \n",
    "        # WHY THESE SETTINGS:\n",
    "        # - 12Hz lowpass: Sleep movements are typically under 10Hz, this removes noise\n",
    "        # - 30Hz resampling: Captures all relevant movement patterns efficiently\n",
    "        # - Non-wear detection: Removes periods when device wasn't being worn\n",
    "        # - Gravity calibration: Ensures consistent measurements regardless of device orientation\n",
    "        \n",
    "        # Night segmentation parameters (how we split data into sleep periods):\n",
    "        self.night_start_hour = 22              # Start of sleep period (10:00 PM)\n",
    "        self.night_end_hour = 6                 # End of sleep period (6:00 AM)\n",
    "        self.night_duration_hours = 8           # Total sleep period duration\n",
    "        \n",
    "        # WHY 22:00-06:00:\n",
    "        # This is the standard 8-hour sleep period used in sleep research\n",
    "        # It captures the main sleep period for most people\n",
    "        \n",
    "        # Quality control parameters (how we filter out bad data):\n",
    "        self.temp_threshold = 17.0              # Temperature threshold in Celsius\n",
    "        \n",
    "        # WHY 17°C:\n",
    "        # When the device is worn against the body, body heat warms it above 17°C\n",
    "        # When it's not worn (on a table, etc.), it stays at room temperature (~20-25°C)\n",
    "        # 17°C is a good threshold to detect when the device was actually being worn\n",
    "        \n",
    "        # Visualization parameters (control which plots to create):\n",
    "        self.create_individual_plots = True     # Create detailed plots for each participant\n",
    "        self.create_summary_plots = True       # Create overall summary plots\n",
    "        \n",
    "        # Initialize the supporting systems\n",
    "        self.setup_logging()                # Set up the system to record what happens\n",
    "        self.initialize_stats()             # Set up counters to track our progress\n",
    "        \n",
    "        # Print information about what mode we're running in\n",
    "        print(f\"Running in {self.mode} mode\")\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        print(f\"Temperature threshold: {self.temp_threshold}°C\")\n",
    "        print(f\"Night period: {self.night_start_hour}:00 - {self.night_end_hour}:00\")\n",
    "    \n",
    "    def initialize_stats(self):\n",
    "        \"\"\"\n",
    "        Set up counters to keep track of processing statistics.\n",
    "        This helps us monitor success rates and identify any problems.\n",
    "        \"\"\"\n",
    "        self.stats = {\n",
    "            'total_files': 0,               # How many .cwa files we found\n",
    "            'processed_files': 0,           # How many files we successfully processed\n",
    "            'failed_files': 0,              # How many files had errors\n",
    "            'total_nights': 0,              # Total number of nights across all files\n",
    "            'valid_nights': 0,              # Number of nights that passed quality control\n",
    "            'total_hours': 0.0,             # Total hours of data processed\n",
    "            'valid_hours': 0.0,             # Hours of data that passed quality control\n",
    "            'controls_processed': 0,        # How many control files processed\n",
    "            'irbd_processed': 0,            # How many iRBD files processed\n",
    "            'temperature_filtered_hours': 0.0,  # Hours removed by temperature filtering\n",
    "            'nonwear_filtered_hours': 0.0   # Hours removed by non-wear detection\n",
    "        }\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"\n",
    "        Set up the logging system to record everything that happens during preprocessing.\n",
    "        This creates a detailed record of the process for debugging and documentation.\n",
    "        \"\"\"\n",
    "        # Create a unique log file name with current date and time\n",
    "        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_file = self.log_dir / f\"preprocessing_{self.mode.lower()}_{current_time}.log\"\n",
    "        \n",
    "        # Configure the logging system\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),      # Save messages to log file\n",
    "                logging.StreamHandler(sys.stdout)   # Also display on screen\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create our logger object\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Write initial log messages\n",
    "        self.logger.info(f\"=== Preprocessing Pipeline Started ({self.mode}) ===\")\n",
    "        self.logger.info(f\"Base directory: {self.base_dir}\")\n",
    "        self.logger.info(f\"Temperature threshold: {self.temp_threshold}°C\")\n",
    "        self.logger.info(f\"Night period: {self.night_start_hour}:00-{self.night_end_hour}:00\")\n",
    "        self.logger.info(f\"actipy parameters: {self.lowpass_hz}Hz lowpass, {self.resample_hz}Hz resample\")\n",
    "    \n",
    "    def find_cwa_files(self, directory):\n",
    "        \"\"\"\n",
    "        Look for all .cwa and .CWA files in a specific directory.\n",
    "        This handles both lowercase and uppercase file extensions.\n",
    "        \n",
    "        Args:\n",
    "            directory: The folder path where we want to search for accelerometer files\n",
    "            \n",
    "        Returns:\n",
    "            A list of file paths for all accelerometer files found in the directory\n",
    "        \"\"\"\n",
    "        # Search for both .cwa and .CWA files (some systems use different cases)\n",
    "        cwa_pattern_lower = directory / \"*.cwa\"\n",
    "        cwa_pattern_upper = directory / \"*.CWA\"\n",
    "        \n",
    "        # Find all files matching both patterns\n",
    "        cwa_files = glob.glob(str(cwa_pattern_lower)) + glob.glob(str(cwa_pattern_upper))\n",
    "        \n",
    "        # Convert file paths from strings to Path objects\n",
    "        cwa_files = [Path(f) for f in cwa_files]\n",
    "        \n",
    "        # Sort files alphabetically for consistent processing order\n",
    "        cwa_files.sort()\n",
    "        \n",
    "        # Log how many files we found\n",
    "        self.logger.info(f\"Found {len(cwa_files)} .cwa/.CWA files in {directory}\")\n",
    "        \n",
    "        return cwa_files\n",
    "    \n",
    "    def read_accelerometer_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Read raw accelerometer data from a .cwa file using actipy.\n",
    "        This applies all the signal processing (filtering, resampling, calibration).\n",
    "        \n",
    "        WHAT ACTIPY DOES:\n",
    "        - Reads the binary .cwa file format\n",
    "        - Applies a 12Hz lowpass filter to remove high-frequency noise\n",
    "        - Resamples the data to 30Hz for consistent sampling rate\n",
    "        - Calibrates for gravity and device orientation\n",
    "        - Detects periods when the device wasn't worn (non-wear detection)\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the .cwa file to read\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (data_dataframe, info_dictionary)\n",
    "                - data_dataframe: Contains x, y, z accelerometer data and timestamps\n",
    "                - info_dictionary: Contains metadata about the processing\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract participant ID from filename (remove .cwa extension)\n",
    "            participant_id = file_path.stem\n",
    "            \n",
    "            self.logger.info(f\"Reading accelerometer data: {participant_id}\")\n",
    "            \n",
    "            # Use actipy to read and process the accelerometer file\n",
    "            # This is where all the signal processing magic happens\n",
    "            data, info = actipy.read_device(\n",
    "                str(file_path),                          # Path to the .cwa file\n",
    "                lowpass_hz=self.lowpass_hz,         # Apply 12Hz lowpass filter\n",
    "                resample_hz=self.resample_hz,       # Resample to 30Hz\n",
    "                detect_nonwear=self.detect_nonwear, # Automatically detect non-wear periods\n",
    "                calibrate_gravity=self.calibrate_gravity  # Correct for gravity and orientation\n",
    "            )\n",
    "            \n",
    "            # Log information about what actipy found\n",
    "            self.logger.info(f\"   - Total samples: {len(data):,}\")\n",
    "            self.logger.info(f\"   - Sampling rate: {info.get('ResampleRate', 'Unknown')} Hz\")\n",
    "            self.logger.info(f\"   - Duration: {info.get('WearTime(days)', 0):.2f} days\")\n",
    "            self.logger.info(f\"   - Non-wear time: {info.get('NonwearTime(days)', 0):.2f} days\")\n",
    "            \n",
    "            return data, info\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If anything goes wrong, log the error and re-raise it\n",
    "            self.logger.error(f\"Error reading {file_path.name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def apply_temperature_filtering(self, data):\n",
    "        \"\"\"\n",
    "        Apply temperature-based filtering to remove periods when the device wasn't worn.\n",
    "        \n",
    "        WHAT TEMPERATURE FILTERING DOES:\n",
    "        When an accelerometer is worn against the body, body heat warms the device.\n",
    "        When it's not worn (sitting on a table, in a drawer), it stays at room temperature.\n",
    "        We use 17°C as the threshold - data below this temperature is likely non-wear.\n",
    "        \n",
    "        WHY 17°C:\n",
    "        - Normal body temperature is ~37°C\n",
    "        - Body heat warms the device to typically 25-35°C when worn\n",
    "        - Room temperature is typically 20-25°C\n",
    "        - 17°C provides a safe margin to detect true non-wear periods\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame containing accelerometer data with temperature column\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Filtered data with only periods above temperature threshold\n",
    "        \"\"\"\n",
    "        # Count how much data we started with\n",
    "        initial_samples = len(data)\n",
    "        initial_hours = initial_samples / self.resample_hz / 3600\n",
    "        \n",
    "        self.logger.info(f\" Applying temperature filtering (≥{self.temp_threshold}°C)\")\n",
    "        self.logger.info(f\"   - Initial data: {initial_samples:,} samples ({initial_hours:.1f} hours)\")\n",
    "        \n",
    "        # Apply temperature threshold - only keep data when temperature ≥ 17°C\n",
    "        # This indicates the device was close to the body (being worn)\n",
    "        temp_mask = data['temperature'] >= self.temp_threshold\n",
    "        \n",
    "        # actipy already set non-wear periods to NaN (Not a Number)\n",
    "        # Find all the data points that are NOT NaN (these are valid data points)\n",
    "        valid_data_mask = ~(data['x'].isna() | data['y'].isna() | data['z'].isna())\n",
    "        \n",
    "        # Combine both filters: data must be warm enough AND not flagged as non-wear\n",
    "        combined_mask = temp_mask & valid_data_mask\n",
    "        \n",
    "        # Apply the combined filter to keep only good quality data\n",
    "        filtered_data = data[combined_mask].copy()\n",
    "        \n",
    "        # Calculate how much data we kept vs removed\n",
    "        final_samples = len(filtered_data)\n",
    "        final_hours = final_samples / self.resample_hz / 3600\n",
    "        removed_samples = initial_samples - final_samples\n",
    "        removed_hours = removed_samples / self.resample_hz / 3600\n",
    "        retention_rate = (final_samples / initial_samples * 100) if initial_samples > 0 else 0\n",
    "        \n",
    "        # Log the results of filtering\n",
    "        self.logger.info(f\"   - After filtering: {final_samples:,} samples ({final_hours:.1f} hours)\")\n",
    "        self.logger.info(f\"   - Removed: {removed_samples:,} samples ({removed_hours:.1f} hours)\")\n",
    "        self.logger.info(f\"   - Retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # Update our statistics\n",
    "        self.stats['temperature_filtered_hours'] += removed_hours\n",
    "        \n",
    "        # Check if we have any data left after filtering\n",
    "        if len(filtered_data) == 0:\n",
    "            raise ValueError(\"No data remaining after temperature filtering\")\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def segment_nights(self, data):\n",
    "        \"\"\"\n",
    "        Split continuous accelerometer data into individual night periods (22:00-06:00).\n",
    "        This version properly handles timestamps that are in the DataFrame index.\n",
    "    \n",
    "        HOW IT WORKS:\n",
    "        1. Converts the DataFrame index to datetime if needed\n",
    "        2. Identifies the date range covered by the data\n",
    "        3. For each date, creates a night period (22:00 to next day 06:00)\n",
    "        4. Extracts data for each night period\n",
    "        5. Returns a list of dictionaries with night information\n",
    "    \n",
    "        KEY IMPROVEMENTS:\n",
    "        - Now correctly handles timestamps stored in DataFrame index\n",
    "        - More robust date/time handling\n",
    "        - Better logging for debugging\n",
    "        - Maintains all original functionality\n",
    "    \n",
    "        Args:\n",
    "            data: Pandas DataFrame with accelerometer data (x,y,z) and timestamps in the index\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries, each containing:\n",
    "            - night_number: Sequential night count (1, 2, 3...)\n",
    "            - date: Calendar date of night start\n",
    "            - start_time: Exact datetime when night starts (22:00)\n",
    "            - end_time: Exact datetime when night ends (06:00 next day)\n",
    "            - data: DataFrame with night's accelerometer data\n",
    "            - samples: Number of samples in this night\n",
    "            - duration_hours: Duration in hours\n",
    "        \"\"\"\n",
    "    \n",
    "        self.logger.info(f\"Segmenting data into night periods ({self.night_start_hour}:00-{self.night_end_hour}:00)\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 1: PREPARE TIMESTAMPS\n",
    "        # =====================================================================\n",
    "        # actipy stores timestamps in the DataFrame index - we need to ensure:\n",
    "        # 1. The index is properly formatted as datetime\n",
    "        # 2. We can access it for night segmentation\n",
    "    \n",
    "        # Convert index to datetime if it isn't already\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            try:\n",
    "                data.index = pd.to_datetime(data.index)\n",
    "                self.logger.info(\"   - Converted index to datetime format\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"   - Failed to convert index to datetime: {str(e)}\")\n",
    "                raise ValueError(\"Could not convert DataFrame index to datetime\")\n",
    "    \n",
    "        # Create a copy of the data to avoid modifying the original\n",
    "        data = data.copy()\n",
    "    \n",
    "        # Add timestamps as a column for easier access (optional)\n",
    "        # This isn't strictly necessary but can make code more readable\n",
    "        data['timestamp'] = data.index\n",
    "    \n",
    "        # =====================================================================\n",
    "        # STEP 2: IDENTIFY DATE RANGE\n",
    "        # =====================================================================\n",
    "        # Find the first and last dates in our data\n",
    "        # Note: .date() converts datetime to just date (without time)\n",
    "    \n",
    "        start_date = data.index.min().date()\n",
    "        end_date = data.index.max().date()\n",
    "    \n",
    "        self.logger.info(f\"   - Data spans from {start_date} to {end_date}\")\n",
    "        self.logger.info(f\"   - First timestamp: {data.index.min()}\")\n",
    "        self.logger.info(f\"   - Last timestamp: {data.index.max()}\")\n",
    "    \n",
    "        # =====================================================================\n",
    "        # STEP 3: EXTRACT NIGHT PERIODS\n",
    "        # =====================================================================\n",
    "        # Initialize list to store each night's data\n",
    "        nights_data = []\n",
    "    \n",
    "        current_date = start_date\n",
    "        night_number = 1  # Count nights sequentially (1, 2, 3...)\n",
    "    \n",
    "        # Loop through each date in the recording period\n",
    "        while current_date <= end_date:\n",
    "            # =============================================================\n",
    "            # STEP 3.1: DEFINE NIGHT BOUNDARIES\n",
    "            # =============================================================\n",
    "            # Night starts at night_start_hour (22:00) on current_date\n",
    "            night_start = datetime.combine(current_date, time(self.night_start_hour, 0))\n",
    "        \n",
    "            # Night ends at night_end_hour (06:00) on the NEXT day\n",
    "            next_date = current_date + timedelta(days=1)\n",
    "            night_end = datetime.combine(next_date, time(self.night_end_hour, 0))\n",
    "        \n",
    "            # =============================================================\n",
    "            # STEP 3.2: EXTRACT DATA FOR THIS NIGHT\n",
    "            # =============================================================\n",
    "            # Create boolean mask for this night period\n",
    "            night_mask = (data.index >= night_start) & (data.index < night_end)\n",
    "            night_data = data[night_mask].copy()\n",
    "        \n",
    "            # =============================================================\n",
    "            # STEP 3.3: STORE NIGHT DATA IF NOT EMPTY\n",
    "            # =============================================================\n",
    "            if len(night_data) > 0:\n",
    "                # Calculate duration in hours\n",
    "                duration_hours = len(night_data) / self.resample_hz / 3600\n",
    "            \n",
    "                # Store night information in dictionary\n",
    "                night_info = {\n",
    "                    'night_number': night_number,\n",
    "                    'date': current_date,\n",
    "                    'start_time': night_start,\n",
    "                    'end_time': night_end,\n",
    "                    'data': night_data,\n",
    "                    'samples': len(night_data),\n",
    "                    'duration_hours': duration_hours\n",
    "                }\n",
    "            \n",
    "                nights_data.append(night_info)\n",
    "            \n",
    "                self.logger.info(f\"   - Night {night_number} ({current_date}): \"\n",
    "                           f\"{len(night_data):,} samples ({duration_hours:.1f}h)\")\n",
    "            \n",
    "                night_number += 1\n",
    "            else:\n",
    "                self.logger.info(f\"   - Night {current_date}: No data available\")\n",
    "        \n",
    "            # Move to next date\n",
    "            current_date = next_date\n",
    "    \n",
    "        # =====================================================================\n",
    "        # STEP 4: FINAL VALIDATION AND STATISTICS\n",
    "        # =====================================================================\n",
    "        total_nights = len(nights_data)\n",
    "        total_night_hours = sum(night['duration_hours'] for night in nights_data)\n",
    "    \n",
    "        self.logger.info(f\"   - Total nights extracted: {total_nights}\")\n",
    "        self.logger.info(f\"   - Total night data: {total_night_hours:.1f} hours\")\n",
    "    \n",
    "        # Validate we found at least one night\n",
    "        if total_nights == 0:\n",
    "            self.logger.warning(\"No night periods found in data!\")\n",
    "    \n",
    "        return nights_data\n",
    "    \n",
    "    def save_preprocessed_data(self, nights_data, participant_id, output_path):\n",
    "        \"\"\"\n",
    "        Save the preprocessed night-segmented data to an HDF5 file.\n",
    "        \n",
    "        WHAT HDF5 IS:\n",
    "        HDF5 is a file format designed for storing large amounts of numerical data.\n",
    "        It's much more efficient than CSV files and can store complex data structures.\n",
    "        It's widely used in scientific computing and data analysis.\n",
    "        \n",
    "        WHY WE USE HDF5:\n",
    "        - Efficient storage of large numerical arrays\n",
    "        - Can store multiple datasets in one file (perfect for multiple nights)\n",
    "        - Fast reading and writing\n",
    "        - Can store metadata (attributes) alongside data\n",
    "        - Widely supported by data analysis tools\n",
    "        \n",
    "        Args:\n",
    "            nights_data: List of dictionaries containing data for each night\n",
    "            participant_id: Unique identifier for this participant\n",
    "            output_path: Full path where to save the HDF5 file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Saving preprocessed data: {output_path.name}\")\n",
    "            \n",
    "            # Create the HDF5 file for writing\n",
    "            with h5py.File(output_path, 'w') as f:\n",
    "                # Save participant information as file attributes (metadata)\n",
    "                f.attrs['name'] = participant_id                    # Participant identifier\n",
    "                f.attrs['number_of_nights'] = len(nights_data)      # How many nights we have\n",
    "                \n",
    "                # Save each night's data as a separate group in the file\n",
    "                for night_info in nights_data:\n",
    "                    night_num = night_info['night_number']\n",
    "                    night_data = night_info['data']\n",
    "                    \n",
    "                    # Create a group for this night (like a folder within the file)\n",
    "                    night_group = f.create_group(f\"night{night_num}\")\n",
    "                    \n",
    "                    # Save the accelerometer data (x, y, z axes)\n",
    "                    night_group.create_dataset('x', data=night_data['x'].values)\n",
    "                    night_group.create_dataset('y', data=night_data['y'].values)\n",
    "                    night_group.create_dataset('z', data=night_data['z'].values)\n",
    "                    \n",
    "                    # Save timestamps as strings (HDF5 handles this efficiently)\n",
    "                    timestamps_str = [ts.isoformat() for ts in night_data['timestamp']]\n",
    "                    night_group.create_dataset('timestamps', data=timestamps_str)\n",
    "            \n",
    "            # Log successful save with file size information\n",
    "            file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "            self.logger.info(f\"   - File saved successfully: {file_size_mb:.1f} MB\")\n",
    "            self.logger.info(f\"   - Nights saved: {len(nights_data)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If anything goes wrong, log the error and re-raise it\n",
    "            self.logger.error(f\"Error saving {output_path.name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def verify_h5_structure(self, h5_file):\n",
    "        \"\"\"\n",
    "        Verify that the saved HDF5 file has the correct structure and no metadata.\n",
    "        This is a quality control check to ensure our files are saved correctly.\n",
    "        \n",
    "        Args:\n",
    "            h5_file: Path to the HDF5 file to verify\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if structure is correct, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                # Check that we have the required attributes\n",
    "                if 'name' not in f.attrs or 'number_of_nights' not in f.attrs:\n",
    "                    self.logger.error(f\"Missing required attributes in {h5_file.name}\")\n",
    "                    return False\n",
    "                \n",
    "                num_nights = f.attrs['number_of_nights']\n",
    "                \n",
    "                # Check each night group\n",
    "                for night_num in range(1, num_nights + 1):\n",
    "                    night_group_name = f\"night{night_num}\"\n",
    "                    \n",
    "                    if night_group_name not in f:\n",
    "                        self.logger.error(f\"Missing {night_group_name} in {h5_file.name}\")\n",
    "                        return False\n",
    "                    \n",
    "                    night_group = f[night_group_name]\n",
    "                    \n",
    "                    # Check that each night has the required datasets\n",
    "                    required_datasets = ['x', 'y', 'z', 'timestamps']\n",
    "                    for dataset_name in required_datasets:\n",
    "                        if dataset_name not in night_group:\n",
    "                            self.logger.error(f\"Missing {dataset_name} in {night_group_name} of {h5_file.name}\")\n",
    "                            return False\n",
    "                    \n",
    "                    # Verify that night group has NO metadata (as requested)\n",
    "                    if len(night_group.attrs) > 0:\n",
    "                        self.logger.error(f\"Unexpected metadata found in {night_group_name} of {h5_file.name}\")\n",
    "                        return False\n",
    "                \n",
    "                self.logger.info(f\"Structure verification passed: {h5_file.name}\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error verifying {h5_file.name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def process_participant(self, cwa_file, group_type):\n",
    "        \"\"\"\n",
    "        Process one participant from start to finish.\n",
    "        This is the main processing function that orchestrates all steps for one person.\n",
    "        \n",
    "        Args:\n",
    "            cwa_file: Path to the participant's .cwa file\n",
    "            group_type: Either 'controls' or 'irbd'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract participant ID from filename\n",
    "            participant_id = cwa_file.stem\n",
    "            \n",
    "            # Log that we're starting to process this participant\n",
    "            self.logger.info(f\"\\n{'='*60}\")\n",
    "            self.logger.info(f\"Processing {group_type.upper()}: {participant_id}\")\n",
    "            self.logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # STEP 1: Read accelerometer data using actipy\n",
    "            accel_data, accel_info = self.read_accelerometer_data(cwa_file)\n",
    "            \n",
    "            # STEP 2: Apply temperature filtering and non-wear removal\n",
    "            filtered_data = self.apply_temperature_filtering(accel_data)\n",
    "            \n",
    "            # STEP 3: Segment data into individual nights\n",
    "            nights_data = self.segment_nights(filtered_data)\n",
    "            \n",
    "            # STEP 4: Save preprocessed data to HDF5 file\n",
    "            if group_type == 'controls':\n",
    "                output_dir = self.preprocessed_controls_dir\n",
    "            else:\n",
    "                output_dir = self.preprocessed_irbd_dir\n",
    "            \n",
    "            output_path = output_dir / f\"{participant_id}.h5\"\n",
    "            self.save_preprocessed_data(nights_data, participant_id, output_path)\n",
    "            \n",
    "            # STEP 5: Verify the saved file structure\n",
    "            if self.verify_h5_structure(output_path):\n",
    "                self.logger.info(f\"{participant_id} processed successfully\")\n",
    "            else:\n",
    "                self.logger.error(f\"Structure verification failed for {participant_id}\")\n",
    "                self.stats['failed_files'] += 1\n",
    "                return\n",
    "            \n",
    "            # STEP 6: Update statistics\n",
    "            self.stats['processed_files'] += 1\n",
    "            self.stats['total_nights'] += len(nights_data)\n",
    "            self.stats['valid_nights'] += len(nights_data)\n",
    "            \n",
    "            total_hours = sum(night['duration_hours'] for night in nights_data)\n",
    "            self.stats['valid_hours'] += total_hours\n",
    "            \n",
    "            if group_type == 'controls':\n",
    "                self.stats['controls_processed'] += 1\n",
    "            else:\n",
    "                self.stats['irbd_processed'] += 1\n",
    "            \n",
    "            # Log success summary\n",
    "            self.logger.info(f\"Processing summary:\")\n",
    "            self.logger.info(f\"   - Nights: {len(nights_data)}\")\n",
    "            self.logger.info(f\"   - Total hours: {total_hours:.1f}\")\n",
    "            self.logger.info(f\"   - Output file: {output_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If anything goes wrong, log the error but continue with other participants\n",
    "            self.logger.error(f\"Error processing {participant_id}: {str(e)}\")\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            self.stats['failed_files'] += 1\n",
    "    \n",
    "    def run_example_test(self):\n",
    "        \"\"\"\n",
    "        Run preprocessing on just one example participant for testing.\n",
    "        This processes the example .cwa file to verify everything works correctly.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"EXAMPLE TEST MODE: Processing example participant\")\n",
    "        \n",
    "        # Check if the example file exists\n",
    "        if not self.raw_example_file.exists():\n",
    "            self.logger.error(f\"Example file not found: {self.raw_example_file}\")\n",
    "            return\n",
    "        \n",
    "        # Determine which group the example file belongs to\n",
    "        # (This is just for logging purposes - the processing is the same)\n",
    "        group_type = 'controls'  # Assume it's a control unless we know otherwise\n",
    "        \n",
    "        # Set statistics for processing one file\n",
    "        self.stats['total_files'] = 1\n",
    "        \n",
    "        # Process the example participant\n",
    "        self.process_participant(self.raw_example_file, group_type)\n",
    "        \n",
    "        # Show final results\n",
    "        self.print_final_statistics()\n",
    "        \n",
    "        # Report success or failure\n",
    "        if self.stats['processed_files'] > 0:\n",
    "            self.logger.info(\"EXAMPLE TEST SUCCESSFUL!\")\n",
    "            self.logger.info(\"Ready for full dataset processing!\")\n",
    "        else:\n",
    "            self.logger.error(\"EXAMPLE TEST FAILED!\")\n",
    "    \n",
    "    def run_full_processing(self):\n",
    "        \"\"\"\n",
    "        Run preprocessing on all participants in the dataset.\n",
    "        This processes all .cwa files in both controls and iRBD directories.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"FULL PROCESSING MODE: Processing all participants\")\n",
    "        \n",
    "        # Find all .cwa files in both directories\n",
    "        controls_files = self.find_cwa_files(self.raw_controls_dir)\n",
    "        irbd_files = self.find_cwa_files(self.raw_irbd_dir)\n",
    "        \n",
    "        # Calculate total number of files\n",
    "        total_files = len(controls_files) + len(irbd_files)\n",
    "        self.stats['total_files'] = total_files\n",
    "        \n",
    "        # Check if we found any files\n",
    "        if total_files == 0:\n",
    "            self.logger.error(\"No .cwa files found in input directories\")\n",
    "            return\n",
    "        \n",
    "        # Log what we found\n",
    "        self.logger.info(f\"Found {total_files} files:\")\n",
    "        self.logger.info(f\"   - Controls: {len(controls_files)} files\")\n",
    "        self.logger.info(f\"   - iRBD: {len(irbd_files)} files\")\n",
    "        \n",
    "        # Process all control files\n",
    "        self.logger.info(f\"\\n Processing CONTROLS ({len(controls_files)} files)...\")\n",
    "        for i, cwa_file in enumerate(controls_files, 1):\n",
    "            self.logger.info(f\"\\n--- Controls Progress: {i}/{len(controls_files)} ---\")\n",
    "            self.process_participant(cwa_file, 'controls')\n",
    "        \n",
    "        # Process all iRBD files\n",
    "        self.logger.info(f\"\\n Processing iRBD ({len(irbd_files)} files)...\")\n",
    "        for i, cwa_file in enumerate(irbd_files, 1):\n",
    "            self.logger.info(f\"\\n--- iRBD Progress: {i}/{len(irbd_files)} ---\")\n",
    "            self.process_participant(cwa_file, 'irbd')\n",
    "        \n",
    "        # Show final statistics\n",
    "        self.print_final_statistics()\n",
    "    \n",
    "    def run_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Main function to run the preprocessing pipeline.\n",
    "        Decides whether to run example test or full processing based on configuration.\n",
    "        \"\"\"\n",
    "        if self.mode == \"EXAMPLE_TEST\":\n",
    "            self.run_example_test()\n",
    "        else:\n",
    "            self.run_full_processing()\n",
    "    \n",
    "    def print_final_statistics(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive summary of the preprocessing results.\n",
    "        Shows processing success rates, data quality metrics, and file statistics.\n",
    "        \"\"\"\n",
    "        # Print header\n",
    "        self.logger.info(f\"\\n{'='*60}\")\n",
    "        self.logger.info(f\"PREPROCESSING COMPLETED ({self.mode})\")\n",
    "        self.logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # File processing statistics\n",
    "        self.logger.info(f\"File Processing:\")\n",
    "        self.logger.info(f\"  - Total files: {self.stats['total_files']}\")\n",
    "        self.logger.info(f\"  - Successfully processed: {self.stats['processed_files']}\")\n",
    "        self.logger.info(f\"  - Failed to process: {self.stats['failed_files']}\")\n",
    "        \n",
    "        # Calculate success rate\n",
    "        if self.stats['total_files'] > 0:\n",
    "            success_rate = self.stats['processed_files'] / self.stats['total_files'] * 100\n",
    "            self.logger.info(f\"  - Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Group breakdown\n",
    "        self.logger.info(f\"Group Breakdown:\")\n",
    "        self.logger.info(f\"  - Controls processed: {self.stats['controls_processed']}\")\n",
    "        self.logger.info(f\"  - iRBD processed: {self.stats['irbd_processed']}\")\n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Data quality statistics\n",
    "        self.logger.info(f\"Data Quality:\")\n",
    "        self.logger.info(f\"  - Total nights: {self.stats['total_nights']}\")\n",
    "        self.logger.info(f\"  - Valid nights: {self.stats['valid_nights']}\")\n",
    "        self.logger.info(f\"  - Valid data hours: {self.stats['valid_hours']:.1f}\")\n",
    "        self.logger.info(f\"  - Temperature filtered hours: {self.stats['temperature_filtered_hours']:.1f}\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        if self.stats['processed_files'] > 0:\n",
    "            avg_nights = self.stats['valid_nights'] / self.stats['processed_files']\n",
    "            avg_hours = self.stats['valid_hours'] / self.stats['processed_files']\n",
    "            self.logger.info(f\"  - Average nights per participant: {avg_nights:.1f}\")\n",
    "            self.logger.info(f\"  - Average hours per participant: {avg_hours:.1f}\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Final success message\n",
    "        if self.stats['processed_files'] > 0:\n",
    "            self.logger.info(\"Preprocessing pipeline completed successfully!\")\n",
    "            if self.mode == \"EXAMPLE_TEST\":\n",
    "                self.logger.info(\"Example data ready for feature extraction\")\n",
    "            else:\n",
    "                self.logger.info(\"All data ready for feature extraction\")\n",
    "        else:\n",
    "            self.logger.error(\"Preprocessing pipeline failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:20:37,004 - INFO - === Preprocessing Pipeline Started (LOCAL_TEST) ===\n",
      "2025-07-25 18:20:37,005 - INFO - Base directory: iRBD-detection\n",
      " Running in LOCAL_TEST mode\n",
      " Base directory: iRBD-detection\n",
      " Plots will be saved to: iRBD-detection/results/preprocessing/visualizations\n",
      "2025-07-25 18:20:37,007 - INFO -  LOCAL TESTING MODE: Processing example file\n",
      "2025-07-25 18:20:37,008 - ERROR -  Example file not found. Please set self.example_file path in configuration.\n",
      "2025-07-25 18:20:37,009 - ERROR - Expected: iRBD-detection/data/raw/2290025_90001_0_0.cwa\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that runs when the script is executed directly.\n",
    "    Creates a PreprocessingPipeline object and runs the entire process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create and run the preprocessing pipeline\n",
    "        pipeline = PreprocessingPipeline()\n",
    "        pipeline.run_preprocessing()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Preprocessing interrupted by user\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Preprocessing failed with error: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# SCRIPT EXECUTION ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Preprocessing)",
   "language": "python",
   "name": "env_preprocessing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
