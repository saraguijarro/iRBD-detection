{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c3042b-3981-46dd-89c9-82f56805daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (Long Short-Term Memory) CLASSIFIER\n",
    "# Train LSTM model to classify participants as iRBD or non-iRBD based on temporal patterns in SSL-Wearables features.\n",
    "\n",
    "\n",
    "## INPUT\n",
    "# Source : Combined feature arrays (from feature extraction stage)\n",
    "# Directory : \n",
    "#    - /work3/s184484/iRBD-detection/data/features/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/features/irbd/\n",
    "# Format : SSL-Wearables 1024-dimensional feature vector (total_windows, 1024)\n",
    "\n",
    "\n",
    "## PIPELINE\n",
    "# 1. Data Loading and Preparation :\n",
    "#    - Load all participant feature files from both groups\n",
    "#    - Create participant-level sequences (multiple nights per participant)\n",
    "#    - Handle variable sequence lengths (different numbers of nights)\n",
    "#    - Split data at participant level (no data leakage between train/test)\n",
    "# 2. Data Preprocessing :\n",
    "#    - Sequence padding to handle variable lengths (pad to maximum length in batch)\n",
    "#    - Create balanced batches for training with collate function\n",
    "#    - Apply masks to handle padded sequences properly\n",
    "# 3. Model Architecture :\n",
    "#    - Input layer : 1024-dimensional feature vectors\n",
    "#    - LSTM layers : Bidirectional LSTM with dropout for regularization\n",
    "#    - Attention mechanism : Single-head temporal attention to weight night importance\n",
    "#    - Dense layers : Fully connected classifier layer\n",
    "#    - Output layer : Binary classification (control vs iRBD)\n",
    "# 4. Training Process :\n",
    "#    - Cross-validation : Stratified participant-level splits (5-fold)\n",
    "#    - Loss function : Binary cross-entropy with class weighting\n",
    "#    - Optimization : Adam optimizer with fixed learning rate\n",
    "#    - Regularization : Dropout, early stopping, L2 regularization\n",
    "#    - Monitoring : Validation loss, accuracy tracking\n",
    "#    - Memory management : Gradient accumulation and memory cleanup\n",
    "# 5. Model Evaluation :\n",
    "#    - Performance metrics : Accuracy, sensitivity, specificity, AUC-ROC\n",
    "#    - Cross-validation results : Per-fold and average performance metrics\n",
    "#    - Attention analysis : Visualization of temporal attention patterns\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# Format : cross-validation results and training logs\n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/results/lstm/evaluation/     # (performance metrics)\n",
    "#    - /work3/s184484/iRBD-detection/results/lstm/training.log   # (training logs)\n",
    "# Structure within directories :\n",
    "# results/lstm/\n",
    "# ├── evaluation/\n",
    "# │   └── cross_validation_results.json   # 5-fold CV performance summary\n",
    "# └── training.log                        # Detailed training log file\n",
    "\n",
    "\n",
    "\n",
    "## VALIDATION\n",
    "# Model Performance Metrics :\n",
    "#    - AUC-ROC : Area under receiver operating characteristic curve\n",
    "#    - Accuracy : Overall classification accuracy\n",
    "#    - Sensitivity : True positive rate (iRBD detection rate)\n",
    "#    - Specificity : True negative rate (control detection rate)\n",
    "#    - Precision : Positive predictive value\n",
    "#    - F1-Score : Harmonic mean of precision and recall\n",
    "# Cross-Validation :\n",
    "#    - Participant-level splits : Ensure no data leakage between participants\n",
    "#    - Stratified sampling : Maintain class balance across splits\n",
    "#    - 5-fold validation : Optional additional validation strategy\n",
    "# Model Robustness :\n",
    "#    - Sequence length handling : Variable-length sequence processing with masking\n",
    "#    - Attention mechanism : Focus on relevant temporal patterns\n",
    "#    - Regularization : Dropout and weight decay to prevent overfitting\n",
    "#    - Early stopping : Prevent overfitting with validation monitoring\n",
    "\n",
    "\n",
    "## PARAMETERS SUMMARY\n",
    "# Model Architecture :\n",
    "#    - Input dimension : 1024 (SSL-Wearables features)\n",
    "#    - Hidden dimension : 128 (LSTM hidden units)\n",
    "#    - Number of layers : 2 (stacked LSTM layers)\n",
    "#    - Bidirectional : True (forward and backward processing)\n",
    "#    - Attention : Single-head attention mechanism\n",
    "#    - Dropout rate : 0.3 (regularization)\n",
    "# Training Parameters :\n",
    "#    - Learning rate : 0.001 (Adam optimizer)\n",
    "#    - Batch size : 1-2 (depending on GPU memory)\n",
    "#    - Max epochs : 100 (with early stopping)\n",
    "#    - Patience : 20 epochs (early stopping patience)\n",
    "#    - Weight decay : 1e-5 (L2 regularization)\n",
    "#    - Gradient clipping : 1.0 (prevent exploding gradients)\n",
    "# Data Parameters :\n",
    "#    - Sequence length : Variable (padded to max length)\n",
    "#    - Feature dimension : 1024 per time step\n",
    "#    - Cross-validation : 5-fold participant-level splits\n",
    "#    - Class balance : Maintained through stratified sampling and class weighting\n",
    "\n",
    "\n",
    "## ENVIRONMENT : env_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf827de4-63a6-4d60-a35b-c647517da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import traceback\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve, classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9682dc-f8e3-440d-bd5e-a2fa8b32dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM with attention mechanism for sequence classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout_rate, bidirectional):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention Layer\n",
    "        attention_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.attention = nn.Linear(attention_input_dim, 1)\n",
    "        \n",
    "        # Dropout and Classifier\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(attention_input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Reshape input for LSTM\n",
    "        x_reshaped = torch.mean(x, dim=2)  # Average across windows\n",
    "        mask_reshaped = mask[:, :, 0] if mask.dim() == 3 else mask\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(x_reshaped)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_scores = self.attention(lstm_out).squeeze(-1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask_reshaped, -1e9)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_output = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        attended_output = self.dropout_layer(attended_output)\n",
    "        class_logits = self.classifier(attended_output)\n",
    "        \n",
    "        return class_logits, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08763ca9-97a8-4e87-95ab-3e0f10eb881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Complete training pipeline for LSTM-based iRBD detection using SSL-Wearables features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the LSTM training pipeline with all necessary configurations.\"\"\"\n",
    "        \n",
    "        # Project directory structure\n",
    "        self.base_dir = Path(\"/work3/s184484/iRBD-detection\")\n",
    "        \n",
    "        # Input directories\n",
    "        self.features_controls_dir = self.base_dir / \"data\" / \"features\" / \"controls\"\n",
    "        self.features_irbd_dir = self.base_dir / \"data\" / \"features\" / \"irbd\"\n",
    "        \n",
    "        # Output directories\n",
    "        self.lstm_results_dir = self.base_dir / \"results\" / \"lstm\"\n",
    "        self.models_dir = self.lstm_results_dir / \"models\"\n",
    "        self.predictions_dir = self.lstm_results_dir / \"predictions\"\n",
    "        self.evaluation_dir = self.lstm_results_dir / \"evaluation\"\n",
    "        self.interpretability_dir = self.lstm_results_dir / \"interpretability\"\n",
    "        self.visualizations_dir = self.base_dir / \"results\" / \"visualizations\"\n",
    "        \n",
    "        # Create directories\n",
    "        for directory in [self.models_dir, self.predictions_dir, \n",
    "                         self.evaluation_dir, self.interpretability_dir, \n",
    "                         self.visualizations_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model architecture parameters\n",
    "        self.input_dim = 1024\n",
    "        self.hidden_dim = 128\n",
    "        self.num_layers = 2\n",
    "        self.num_classes = 2\n",
    "        self.dropout_rate = 0.3\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        # Training parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 1\n",
    "        self.max_epochs = 100\n",
    "        self.patience = 15\n",
    "        self.weight_decay = 1e-5\n",
    "        self.gradient_clip = 1.0\n",
    "        \n",
    "        # Data parameters\n",
    "        self.min_nights = 5\n",
    "        self.max_sequence_length = 50\n",
    "        self.test_size = 0.2\n",
    "        self.val_size = 0.2\n",
    "        self.cv_folds = 5\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Data storage\n",
    "        self.all_participants = {}\n",
    "        self.participant_labels = {}\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        self.logger.info(\"LSTM Training Pipeline initialized successfully\")\n",
    "        self.logger.info(f\"Project directory: {self.base_dir}\")\n",
    "        self.logger.info(f\"Device: {self.device}\")\n",
    "        self.logger.info(f\"Model architecture: {self.num_layers}-layer {'bidirectional' if self.bidirectional else 'unidirectional'} LSTM with attention\")\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup comprehensive logging for the training pipeline.\"\"\"\n",
    "        \n",
    "        # Create logs directory\n",
    "        logs_dir = self.base_dir / \"jobs\" / \"logs\" / \"lstm\"\n",
    "        logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup logger\n",
    "        self.logger = logging.getLogger('LSTMTraining')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Clear existing handlers\n",
    "        self.logger.handlers.clear()\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler\n",
    "        log_file = logs_dir / f\"lstm_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        \n",
    "        # Add handlers\n",
    "        self.logger.addHandler(console_handler)\n",
    "        self.logger.addHandler(file_handler)\n",
    "    \n",
    "    def load_all_participants(self):\n",
    "        \"\"\"Load all participant feature files with proper dictionary handling.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Loading participant data...\")\n",
    "            \n",
    "            # Load control participants\n",
    "            controls_files = list(self.features_controls_dir.glob(\"*.npy\"))[:5]  # Limit for testing\n",
    "            # controls_files = list(self.features_controls_dir.glob(\"*.npy\"))[:10]  # Limit for testing\n",
    "            self.logger.info(f\"Found {len(controls_files)} control participant files\")\n",
    "            \n",
    "            for feature_file in controls_files:\n",
    "                try:\n",
    "                    # Load the numpy file (0-dimensional array containing a dict)\n",
    "                    data = np.load(feature_file, allow_pickle=True)\n",
    "                    \n",
    "                    # Extract the dictionary from the 0-dimensional array\n",
    "                    if data.shape == ():\n",
    "                        content = data.item()\n",
    "                    else:\n",
    "                        content = data\n",
    "                    \n",
    "                    # Extract the relevant data from the dictionary\n",
    "                    if isinstance(content, dict):\n",
    "                        features = content['features']\n",
    "                        windows_mask = content['windows_mask']\n",
    "                        participant_id = content['participant_id']\n",
    "                        num_nights = content['num_nights']\n",
    "                    else:\n",
    "                        # Fallback for old format\n",
    "                        features = content\n",
    "                        participant_id = feature_file.stem.replace('_features', '')\n",
    "                        num_nights = features.shape[0] if len(features.shape) > 2 else 1\n",
    "                        windows_mask = np.ones((features.shape[0], features.shape[1]), dtype=bool)\n",
    "                    \n",
    "                    # Validate the data\n",
    "                    if len(features.shape) != 3 or features.shape[2] != self.input_dim:\n",
    "                        self.logger.warning(f\"Skipping {participant_id}: Invalid feature shape {features.shape}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if num_nights < self.min_nights:\n",
    "                        self.logger.warning(f\"Skipping {participant_id}: Only {num_nights} nights (minimum: {self.min_nights})\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Store the participant data\n",
    "                    self.all_participants[participant_id] = {\n",
    "                        'features': features,\n",
    "                        'mask': windows_mask,\n",
    "                        'num_nights': num_nights,\n",
    "                        'group': 'control'\n",
    "                    }\n",
    "                    self.participant_labels[participant_id] = 0  # Control = 0\n",
    "                    \n",
    "                    self.logger.debug(f\"Loaded control {participant_id}: {features.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error loading control file {feature_file}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Load iRBD participants\n",
    "            irbd_files = list(self.features_irbd_dir.glob(\"*.npy\"))[:5]  # Limit for testing\n",
    "            #irbd_files = list(self.features_irbd_dir.glob(\"*.npy\"))[:10]  # Limit for testing\n",
    "            self.logger.info(f\"Found {len(irbd_files)} iRBD participant files\")\n",
    "            \n",
    "            for feature_file in irbd_files:\n",
    "                try:\n",
    "                    # Load the numpy file (0-dimensional array containing a dict)\n",
    "                    data = np.load(feature_file, allow_pickle=True)\n",
    "                    \n",
    "                    # Extract the dictionary from the 0-dimensional array\n",
    "                    if data.shape == ():\n",
    "                        content = data.item()\n",
    "                    else:\n",
    "                        content = data\n",
    "                    \n",
    "                    # Extract the relevant data from the dictionary\n",
    "                    if isinstance(content, dict):\n",
    "                        features = content['features']\n",
    "                        windows_mask = content['windows_mask']\n",
    "                        participant_id = content['participant_id']\n",
    "                        num_nights = content['num_nights']\n",
    "                    else:\n",
    "                        # Fallback for old format\n",
    "                        features = content\n",
    "                        participant_id = feature_file.stem.replace('_features', '')\n",
    "                        num_nights = features.shape[0] if len(features.shape) > 2 else 1\n",
    "                        windows_mask = np.ones((features.shape[0], features.shape[1]), dtype=bool)\n",
    "                    \n",
    "                    # Validate the data\n",
    "                    if len(features.shape) != 3 or features.shape[2] != self.input_dim:\n",
    "                        self.logger.warning(f\"Skipping {participant_id}: Invalid feature shape {features.shape}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if num_nights < self.min_nights:\n",
    "                        self.logger.warning(f\"Skipping {participant_id}: Only {num_nights} nights (minimum: {self.min_nights})\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Store the participant data\n",
    "                    self.all_participants[participant_id] = {\n",
    "                        'features': features,\n",
    "                        'mask': windows_mask,\n",
    "                        'num_nights': num_nights,\n",
    "                        'group': 'irbd'\n",
    "                    }\n",
    "                    self.participant_labels[participant_id] = 1  # iRBD = 1\n",
    "                    \n",
    "                    self.logger.info(f\"Loaded iRBD {participant_id}: {features.shape}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error loading iRBD file {feature_file}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Summary\n",
    "            controls_loaded = sum(1 for p in self.all_participants.values() if p['group'] == 'control')\n",
    "            irbd_loaded = sum(1 for p in self.all_participants.values() if p['group'] == 'irbd')\n",
    "            \n",
    "            self.logger.info(f\"Successfully loaded {len(self.all_participants)} participants:\")\n",
    "            self.logger.info(f\"  - Controls: {controls_loaded}\")\n",
    "            self.logger.info(f\"  - iRBD: {irbd_loaded}\")\n",
    "            \n",
    "            if len(self.all_participants) == 0:\n",
    "                self.logger.error(\"No participants loaded! Check feature file paths.\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in load_all_participants: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def create_data_loader(self, participant_ids, batch_size, shuffle=True):\n",
    "        \"\"\"Create a PyTorch DataLoader for the given participants.\"\"\"\n",
    "        \n",
    "        # Create custom dataset class\n",
    "        class ParticipantDataset(Dataset):\n",
    "            def __init__(self, participant_ids, all_participants, participant_labels):\n",
    "                self.participant_ids = participant_ids\n",
    "                self.all_participants = all_participants\n",
    "                self.participant_labels = participant_labels\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.participant_ids)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                participant_id = self.participant_ids[idx]\n",
    "                participant_data = self.all_participants[participant_id]  # This is a dictionary\n",
    "                label = self.participant_labels[participant_id]\n",
    "                \n",
    "                # Extract the actual features array from the dictionary\n",
    "                if isinstance(participant_data, dict):\n",
    "                    features = participant_data['features']  # Shape: (nights, windows, features)\n",
    "                    mask = participant_data['mask']          # Shape: (nights, windows)\n",
    "                else:\n",
    "                    # Fallback for old format\n",
    "                    features = participant_data\n",
    "                    mask = None\n",
    "                \n",
    "                # Convert to tensors\n",
    "                features_tensor = torch.FloatTensor(features)\n",
    "                label_tensor = torch.LongTensor([label])\n",
    "                \n",
    "                return features_tensor, label_tensor, participant_id\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = ParticipantDataset(participant_ids, self.all_participants, self.participant_labels)\n",
    "        \n",
    "        # Custom collate function to handle variable sequence lengths\n",
    "        def collate_fn(batch):\n",
    "            features, labels, participant_ids = zip(*batch)\n",
    "            \n",
    "            # Get the maximum dimensions across all sequences\n",
    "            max_nights = max(f.shape[0] for f in features)\n",
    "            max_windows = max(f.shape[1] for f in features)\n",
    "            feature_dim = features[0].shape[2]  # Should be 1024\n",
    "            \n",
    "            # Pad sequences to the same length\n",
    "            padded_features = []\n",
    "            masks = []\n",
    "            \n",
    "            for f in features:\n",
    "                # Pad the feature tensor\n",
    "                if f.shape[0] < max_nights:\n",
    "                    padding = torch.zeros(max_nights - f.shape[0], f.shape[1], feature_dim)\n",
    "                    padded_f = torch.cat([f, padding], dim=0)\n",
    "                else:\n",
    "                    padded_f = f\n",
    "                \n",
    "                # Pad windows dimension\n",
    "                if f.shape[1] < max_windows:\n",
    "                    window_padding = torch.zeros(padded_f.shape[0], max_windows - f.shape[1], feature_dim)\n",
    "                    padded_f = torch.cat([padded_f, window_padding], dim=1)\n",
    "        \n",
    "                padded_features.append(padded_f) \n",
    "                \n",
    "                # Create mask (True for real data, False for padding)\n",
    "                mask = torch.ones(max_nights, max_windows, dtype=torch.bool)\n",
    "                mask[:f.shape[0], :f.shape[1]] = True\n",
    "                mask[f.shape[0]:, :] = False\n",
    "                mask[:, f.shape[1]:] = False\n",
    "                masks.append(mask)\n",
    "            \n",
    "            # Stack into batches\n",
    "            batch_features = torch.stack(padded_features)\n",
    "            batch_masks = torch.stack(masks)\n",
    "            batch_labels = torch.cat(labels)\n",
    "            batch_lengths = torch.LongTensor([f.shape[0] for f in features])\n",
    "            \n",
    "            # ALWAYS return exactly 5 values in this order:\n",
    "            return batch_features, batch_labels, batch_lengths, batch_masks, participant_ids\n",
    "        \n",
    "        # Create data loader\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0  # Set to 0 for HPC compatibility\n",
    "        )\n",
    "        \n",
    "        return data_loader\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, fold_num=None):\n",
    "        \"\"\"Train the LSTM model with early stopping and comprehensive monitoring.\"\"\"\n",
    "\n",
    "        # Memory monitoring\n",
    "        initial_memory = psutil.Process().memory_info().rss / 1024 ** 2\n",
    "        self.logger.info(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize model\n",
    "            model = LSTMWithAttention(\n",
    "                input_dim=self.input_dim,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_layers=self.num_layers,\n",
    "                num_classes=self.num_classes,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                bidirectional=self.bidirectional\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Calculate class weights\n",
    "            labels = list(self.participant_labels.values())\n",
    "            unique_classes = np.unique(labels)\n",
    "            \n",
    "            if len(unique_classes) == 1:\n",
    "                class_weights = np.array([1.0])\n",
    "                self.logger.warning(f\"Only one class present: {unique_classes[0]}\")\n",
    "            else:\n",
    "                class_weights = compute_class_weight('balanced', classes=unique_classes, y=labels)\n",
    "                self.logger.info(f\"Class weights: Control={class_weights[0]:.3f}, iRBD={class_weights[1]:.3f}\")\n",
    "            \n",
    "            class_weights_tensor = torch.FloatTensor(class_weights).to(self.device)\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "            \n",
    "            # Optimizer\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                weight_decay=self.weight_decay\n",
    "            )\n",
    "            \n",
    "            # Training loop\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            train_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "            \n",
    "            for epoch in range(self.max_epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for features, labels, lengths, masks, participant_ids in train_loader:\n",
    "\n",
    "                    accumulation_steps = 2 # Accumulate over 2 batches\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    for i, (features, labels, lengths, masks, participant_ids) in enumerate(train_loader):\n",
    "                        features = features.to(self.device)\n",
    "                        masks = masks.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                    \n",
    "                        # Forward pass\n",
    "                        outputs, attention_weights = model(features, masks)\n",
    "                        loss = criterion(outputs, labels) / accumulation_steps # Scale loss\n",
    "                        loss.backward()\n",
    "\n",
    "                        # Track metrics\n",
    "                        train_loss += loss.item() * accumulation_steps # Unscale for logging\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        train_correct += (predicted == labels).sum().item()\n",
    "                        train_total += labels.size(0)\n",
    "\n",
    "                        #Clear intermediate tensors\n",
    "                        del features, masks, labels, outputs, attention_weights\n",
    "\n",
    "                        # Update weights every accumulation_step\n",
    "                        if (i + 1) % accumulation_steps == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), self.gradient_clip)\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            # Memory cleanup\n",
    "                            gc.collect()\n",
    "                \n",
    "                    # Handle remaining gradients if batch doesn't divide evenly\n",
    "                    if len(train_loader) % accumulation_steps != 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.gradient_clip)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Memory cleanup\n",
    "                        gc.collect()\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for features, labels, lengths, masks, participant_ids in val_loader:\n",
    "                        features = features.to(self.device)\n",
    "                        masks = masks.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        \n",
    "                        outputs, attention_weights = model(features, masks)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                        val_total += labels.size(0)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_loss /= len(train_loader)\n",
    "                val_loss /= len(val_loader)\n",
    "                train_acc = train_correct / train_total if train_total > 0 else 0\n",
    "                val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "                \n",
    "                # Store history\n",
    "                train_history['train_loss'].append(train_loss)\n",
    "                train_history['val_loss'].append(val_loss)\n",
    "                train_history['train_acc'].append(train_acc)\n",
    "                train_history['val_acc'].append(val_acc)\n",
    "                \n",
    "                # Log progress\n",
    "                if epoch % 10 == 0 or epoch < 5:\n",
    "                    self.logger.info(f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "                                   f\"Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= self.patience:\n",
    "                    self.logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            # Load best model\n",
    "            model.load_state_dict(best_model_state)\n",
    "            \n",
    "            return model, train_history\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in train_model: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def evaluate_model(self, model, data_loader, phase_name=\"Test\"):\n",
    "        \"\"\"Evaluate the model and return comprehensive metrics.\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_participant_ids = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels, lengths, masks, participant_ids in data_loader:\n",
    "                features = features.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs, attention_weights = model(features, masks)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "                all_participant_ids.extend(participant_ids)\n",
    "                all_attention_weights.extend(attention_weights.cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_probabilities = np.array(all_probabilities)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        # Clinical metrics\n",
    "        if len(np.unique(all_labels)) == 2:\n",
    "            sensitivity = recall_score(all_labels, all_predictions, pos_label=1)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_labels, all_predictions).ravel()\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            auc_roc = roc_auc_score(all_labels, all_probabilities[:, 1])\n",
    "            auc_pr = average_precision_score(all_labels, all_probabilities[:, 1])\n",
    "        else:\n",
    "            sensitivity = specificity = auc_roc = auc_pr = 0\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'auc_roc': auc_roc,\n",
    "            'auc_pr': auc_pr,\n",
    "            'confusion_matrix': cm,\n",
    "            'predictions': all_predictions,\n",
    "            'probabilities': all_probabilities,\n",
    "            'labels': all_labels,\n",
    "            'participant_ids': all_participant_ids,\n",
    "            'attention_weights': all_attention_weights\n",
    "        }\n",
    "        \n",
    "        # Log results\n",
    "        self.logger.info(f\"\\n{phase_name} Results:\")\n",
    "        self.logger.info(f\"  Accuracy: {accuracy:.3f}\")\n",
    "        self.logger.info(f\"  Precision: {precision:.3f}\")\n",
    "        self.logger.info(f\"  Recall: {recall:.3f}\")\n",
    "        self.logger.info(f\"  F1-Score: {f1:.3f}\")\n",
    "        if len(np.unique(all_labels)) == 2:\n",
    "            self.logger.info(f\"  Sensitivity: {sensitivity:.3f}\")\n",
    "            self.logger.info(f\"  Specificity: {specificity:.3f}\")\n",
    "            self.logger.info(f\"  AUC-ROC: {auc_roc:.3f}\")\n",
    "            self.logger.info(f\"  AUC-PR: {auc_pr:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_cross_validation(self):\n",
    "        \"\"\"Run stratified cross-validation.\"\"\"\n",
    "        \n",
    "        # Get participant IDs and labels\n",
    "        participant_ids = np.array(list(self.participant_labels.keys()))\n",
    "        labels = np.array(list(self.participant_labels.values()))\n",
    "        \n",
    "        # Stratified K-fold\n",
    "        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        cv_results = []\n",
    "        \n",
    "        for fold_num, (train_val_idx, test_idx) in enumerate(skf.split(participant_ids, labels), 1):\n",
    "            self.logger.info(f\"\\n{'='*60}\")\n",
    "            self.logger.info(f\"CROSS-VALIDATION FOLD {fold_num}/{self.cv_folds}\")\n",
    "            self.logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # Split participants\n",
    "            train_val_participants = participant_ids[train_val_idx]\n",
    "            test_participants = participant_ids[test_idx]\n",
    "            train_val_labels = labels[train_val_idx]\n",
    "            \n",
    "            # Further split train_val into train and validation\n",
    "            train_participants, val_participants, _, _ = train_test_split(\n",
    "                train_val_participants, train_val_labels,\n",
    "                test_size=self.val_size, stratify=train_val_labels, random_state=42\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Fold {fold_num} participants:\")\n",
    "            self.logger.info(f\"  Train: {len(train_participants)} participants\")\n",
    "            self.logger.info(f\"  Validation: {len(val_participants)} participants\")\n",
    "            self.logger.info(f\"  Test: {len(test_participants)} participants\")\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_loader = self.create_data_loader(train_participants, self.batch_size, shuffle=True)\n",
    "            val_loader = self.create_data_loader(val_participants, self.batch_size, shuffle=False)\n",
    "            test_loader = self.create_data_loader(test_participants, self.batch_size, shuffle=False)\n",
    "            \n",
    "            # Train model\n",
    "            self.logger.info(f\"Starting model training (Fold {fold_num})...\")\n",
    "            model, fold_history = self.train_model(train_loader, val_loader, fold_num)\n",
    "            \n",
    "            if model is None:\n",
    "                self.logger.error(f\"Training failed for fold {fold_num}\")\n",
    "                continue\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_results = self.evaluate_model(model, test_loader, f\"Fold {fold_num} Test\")\n",
    "            \n",
    "            # Store results\n",
    "            fold_result = {\n",
    "                'fold_num': fold_num,\n",
    "                'train_participants': train_participants.tolist(),\n",
    "                'val_participants': val_participants.tolist(),\n",
    "                'test_participants': test_participants.tolist(),\n",
    "                'test_results': test_results,\n",
    "                'training_history': fold_history\n",
    "            }\n",
    "            cv_results.append(fold_result)\n",
    "        \n",
    "        return cv_results\n",
    "    \n",
    "    def run_lstm_training(self):\n",
    "        \"\"\"Main function to run the complete LSTM training pipeline.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(\"=\"*80)\n",
    "            self.logger.info(\"STARTING LSTM TRAINING PIPELINE FOR iRBD DETECTION\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            \n",
    "            # Load data\n",
    "            if not self.load_all_participants():\n",
    "                self.logger.error(\"Failed to load participant data\")\n",
    "                return False\n",
    "            \n",
    "            # Run cross-validation\n",
    "            self.logger.info(\"Starting 5-fold cross-validation...\")\n",
    "            cv_results = self.run_cross_validation()\n",
    "            \n",
    "            if not cv_results:\n",
    "                self.logger.error(\"Cross-validation failed\")\n",
    "                return False\n",
    "            \n",
    "            # Calculate average performance\n",
    "            test_accuracies = [fold['test_results']['accuracy'] for fold in cv_results]\n",
    "            test_aucs = [fold['test_results']['auc_roc'] for fold in cv_results if fold['test_results']['auc_roc'] > 0]\n",
    "            \n",
    "            self.logger.info(f\"\\n{'='*80}\")\n",
    "            self.logger.info(\"CROSS-VALIDATION SUMMARY\")\n",
    "            self.logger.info(f\"{'='*80}\")\n",
    "            self.logger.info(f\"Average Test Accuracy: {np.mean(test_accuracies):.3f} ± {np.std(test_accuracies):.3f}\")\n",
    "            if test_aucs:\n",
    "                self.logger.info(f\"Average Test AUC-ROC: {np.mean(test_aucs):.3f} ± {np.std(test_aucs):.3f}\")\n",
    "            \n",
    "            # Save results\n",
    "            results_file = self.evaluation_dir / \"cross_validation_results.json\"\n",
    "            with open(results_file, 'w') as f:\n",
    "                # Convert numpy arrays to lists for JSON serialization\n",
    "                cv_results_serializable = self.convert_numpy_to_list(cv_results)\n",
    "                json.dump(cv_results_serializable, f, indent=2)\n",
    "            \n",
    "            self.logger.info(f\"Results saved to: {results_file}\")\n",
    "            self.logger.info(\"LSTM training pipeline completed successfully!\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"LSTM training pipeline failed: {str(e)}\")\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "    def convert_numpy_to_list(self, obj):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self.convert_numpy_to_list(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_numpy_to_list(item) for item in obj]\n",
    "        elif isinstance(obj, (np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839fe8b3-151e-41d1-981c-c5953e6f6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the LSTM training pipeline.\"\"\"\n",
    "    try:\n",
    "        pipeline = LSTMTrainingPipeline()\n",
    "        pipeline.run_lstm_training()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nLSTM training failed with error: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
