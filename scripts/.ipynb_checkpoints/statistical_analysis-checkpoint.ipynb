{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f5738-159e-4a22-989a-2ff34b3c244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES STATISTICAL ANALYSIS\n",
    "# Statistical analysis of SSL-Wearables features between controls and iRBD groups.\n",
    "# Statistically check if the average features across the night is different between iRBD and controls.\n",
    "\n",
    "## INPUT\n",
    "# Source : SSL-Wearables feature vectors from feature extraction pipeline\n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/data/features/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/features/irbd/\n",
    "# Format : .npy files with night-structured features (nights Ã— windows_per_night Ã— 1024)\n",
    "\n",
    "\n",
    "## PIPELINE\n",
    "# 1. Feature Loading and Aggregation :\n",
    "#    - Load individual participant feature files from both groups\n",
    "#    - Calculate participant-level feature averages (average across all nights and windows)\n",
    "#    - Create two groups: controls vs iRBD participants\n",
    "#    - Ensure proper data structure for statistical testing\n",
    "# 2. Statistical Testing Framework :\n",
    "#    - Test normality for each of the 1024 features using Shapiro-Wilk test\n",
    "#    - Apply appropriate statistical tests based on normality:\n",
    "#         - Independent t-test for normally distributed features\n",
    "#         - Mann-Whitney U test for non-normally distributed features\n",
    "#    - Calculate effect sizes (Cohen's d for t-tests, rank-biserial correlation for Mann-Whitney)\n",
    "#    - Apply Bonferroni correction for multiple comparisons (Î± = 0.05/1024)\n",
    "# 3. Effect Size Analysis :\n",
    "#    - Identify features with large effect sizes (Cohen's d â‰¥ 0.5)\n",
    "#    - Rank features by effect size to find most discriminative patterns\n",
    "#    - Combine statistical significance with practical significance\n",
    "# 4. Results Interpretation :\n",
    "#    - Identify features that are both statistically significant AND have large effect sizes\n",
    "#    - Provide clinical interpretation of significant movement pattern differences\n",
    "#    - Generate comprehensive summary statistics and visualizations\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# Format : Statistical results, effect sizes, and comprehensive visualizations\n",
    "# Directories :\n",
    "#    - /work3/s184484/iRBD-detection/results/statistical_analysis/\n",
    "#    - /work3/s184484/iRBD-detection/results/visualizations/\n",
    "\n",
    "\n",
    "## ENVIRONMENT : env_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdaf7de-4653-4551-9293-7bada184932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python libraries for file operations and system control\n",
    "import os                    # Operating System interface - helps us work with files and folders\n",
    "import sys                   # System-specific parameters - helps us control the program execution\n",
    "import numpy as np           # NumPy - for mathematical operations on arrays of numbers\n",
    "import pandas as pd          # Pandas - for working with data tables and organizing information\n",
    "from datetime import datetime, timedelta  # For working with dates and times\n",
    "import logging               # For creating detailed log files that record what the program does\n",
    "from pathlib import Path     # For easier and more reliable file path handling\n",
    "import glob                  # For finding files that match specific patterns\n",
    "import traceback             # For showing detailed error messages when something goes wrong\n",
    "import json                  # For saving and loading JSON files (configuration and metadata)\n",
    "import pickle                # For saving Python objects (like statistical results)\n",
    "import gc                    # Garbage collection - for managing memory usage\n",
    "import warnings              # For controlling warning messages\n",
    "warnings.filterwarnings('ignore')  # Suppress unnecessary warnings\n",
    "\n",
    "# Statistical analysis libraries\n",
    "import scipy.stats as stats  # For statistical tests (t-tests, Mann-Whitney U, etc.)\n",
    "from scipy.stats import (    # Specific statistical functions we'll use\n",
    "    shapiro,                 # Shapiro-Wilk test for checking if data is normally distributed\n",
    "    mannwhitneyu,           # Mann-Whitney U test (non-parametric alternative to t-test)\n",
    "    ttest_ind,              # Independent t-test for comparing two groups\n",
    "    pearsonr,               # Pearson correlation coefficient\n",
    "    spearmanr,              # Spearman correlation (non-parametric)\n",
    "    normaltest,             # D'Agostino's normality test (alternative to Shapiro-Wilk)\n",
    "    levene                  # Levene's test for equal variances\n",
    ")\n",
    "from statsmodels.stats.multitest import multipletests  # Multiple comparison correction methods\n",
    "import statsmodels.api as sm  # Statistical models for advanced analysis\n",
    "\n",
    "# WHAT IS STATISTICAL TESTING:\n",
    "# Statistical testing helps us determine if differences between groups are real\n",
    "# or just due to random chance. We use p-values to quantify this uncertainty.\n",
    "\n",
    "# WHAT IS EFFECT SIZE:\n",
    "# Effect size tells us how big the difference is between groups, regardless of\n",
    "# statistical significance. Large effect sizes indicate practically meaningful differences.\n",
    "\n",
    "# WHAT IS MULTIPLE COMPARISON CORRECTION:\n",
    "# When testing many features (1024), we need to adjust our significance threshold\n",
    "# to avoid finding false positives due to chance alone.\n",
    "\n",
    "# Machine learning libraries for data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler  # For normalizing data (z-score standardization)\n",
    "from sklearn.decomposition import PCA            # Principal Component Analysis for dimensionality reduction\n",
    "from sklearn.manifold import TSNE                # t-SNE for visualization of high-dimensional data\n",
    "\n",
    "# Visualization libraries for creating plots and charts\n",
    "import matplotlib.pyplot as plt    # Main plotting library - like creating graphs in Excel\n",
    "import seaborn as sns             # Statistical plotting library - makes beautiful, professional plots\n",
    "\n",
    "# Configure matplotlib and seaborn for professional-looking plots\n",
    "plt.style.use('seaborn-v0_8')     # Use seaborn's visual style (makes plots look professional)\n",
    "sns.set_palette(\"husl\")           # Set a nice color palette (colors that work well together)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)  # Set default size for all plots (12 inches wide, 8 inches tall)\n",
    "plt.rcParams['font.size'] = 10    # Set default font size for all text in plots\n",
    "plt.rcParams['axes.grid'] = True  # Show grid lines on plots for easier reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4268a1-1348-4c05-9a41-c92958338fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS PIPELINE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureStatisticalAnalysis:\n",
    "    \"\"\"\n",
    "    Comprehensive statistical analysis pipeline for comparing SSL-Wearables features \n",
    "    between iRBD patients and healthy controls.\n",
    "    \n",
    "    WHAT THIS CLASS DOES:\n",
    "    This class performs a thorough statistical comparison of movement patterns between\n",
    "    iRBD patients and healthy controls using the 1024-dimensional SSL-Wearables features.\n",
    "    It identifies which specific movement characteristics are significantly different\n",
    "    between the two groups and quantifies the size of these differences.\n",
    "    \n",
    "    WHY THIS ANALYSIS IS IMPORTANT:\n",
    "    - Identifies specific movement patterns that distinguish iRBD from normal sleep\n",
    "    - Provides scientific evidence for the effectiveness of the detection approach\n",
    "    - Helps understand the underlying pathophysiology of iRBD\n",
    "    - Validates that the SSL-Wearables features capture clinically relevant information\n",
    "    - Supports the development of better diagnostic tools\n",
    "    \n",
    "    STATISTICAL APPROACH:\n",
    "    1. Calculate participant-level averages (reduces night-to-night variability)\n",
    "    2. Test each feature for normality to choose appropriate statistical tests\n",
    "    3. Compare groups using t-tests or Mann-Whitney U tests as appropriate\n",
    "    4. Calculate effect sizes to quantify practical significance\n",
    "    5. Apply multiple comparison correction to control false discovery rate\n",
    "    6. Identify features with both statistical and practical significance\n",
    "    \n",
    "    NOTE: This analysis requires multiple participants per group (controls and iRBD).\n",
    "    It cannot be tested with single example files like preprocessing or feature extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the statistical analysis pipeline with all necessary configuration.\n",
    "        This sets up directories, parameters, and statistical frameworks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # =================================================================\n",
    "        # DIRECTORY SETUP - Define where to find files and save results\n",
    "        # =================================================================\n",
    "        \n",
    "        # Main project directory on HPC\n",
    "        self.base_dir = Path(\"/work3/s184484/iRBD-detection\")\n",
    "        \n",
    "        # Input directories (where the features are stored):\n",
    "        self.features_dir = self.base_dir / \"data\" / \"features\"                    # Main features directory\n",
    "        self.features_controls_dir = self.features_dir / \"controls\"               # Individual control features\n",
    "        self.features_irbd_dir = self.features_dir / \"irbd\"                       # Individual iRBD features\n",
    "        \n",
    "        # Output directories (where to save statistical analysis results):\n",
    "        self.results_dir = self.base_dir / \"results\"                              # Main results directory\n",
    "        self.stats_results_dir = self.results_dir / \"statistical_analysis\"       # Statistical analysis results\n",
    "        \n",
    "        # Visualization output directory (where to save plots for the report):\n",
    "        self.plots_dir = self.results_dir / \"visualizations\"\n",
    "        \n",
    "        # Only create log directory if it doesn't exist (for logging only)\n",
    "        self.log_dir = self.base_dir / \"validation\" / \"data_quality_reports\"\n",
    "        if not self.log_dir.exists():\n",
    "            self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # =================================================================\n",
    "        # STATISTICAL PARAMETERS - Settings that control the analysis\n",
    "        # =================================================================\n",
    "        \n",
    "        # Multiple comparison correction parameters:\n",
    "        self.alpha = 0.05                           # Overall significance level (5%)\n",
    "        self.feature_dim = 1024                     # Number of SSL-Wearables features to test\n",
    "        self.bonferroni_alpha = self.alpha / self.feature_dim  # Bonferroni corrected alpha\n",
    "        \n",
    "        # WHY BONFERRONI CORRECTION:\n",
    "        # When testing 1024 features simultaneously, we have a high chance of finding\n",
    "        # false positives by random chance alone. Bonferroni correction divides our\n",
    "        # significance threshold by the number of tests to control this.\n",
    "        # New threshold: 0.05/1024 = 4.88e-05 (much more stringent)\n",
    "        \n",
    "        # Effect size thresholds (Cohen's conventions for interpreting effect sizes):\n",
    "        self.small_effect = 0.2                     # Small effect size (subtle difference)\n",
    "        self.medium_effect = 0.5                    # Medium effect size (moderate difference)\n",
    "        self.large_effect = 0.8                     # Large effect size (substantial difference)\n",
    "        \n",
    "        # WHY COHEN'S d â‰¥ 0.5 AS THRESHOLD:\n",
    "        # Medium effect sizes (â‰¥0.5) are generally considered clinically meaningful.\n",
    "        # This ensures we focus on features with practical, not just statistical, significance.\n",
    "        # Small effects might be statistically significant but not clinically useful.\n",
    "        \n",
    "        # Normality testing parameters:\n",
    "        self.normality_alpha = 0.05                 # Significance level for normality tests\n",
    "        self.min_sample_size = 5                    # Minimum sample size for statistical tests\n",
    "        \n",
    "        # WHY TEST FOR NORMALITY:\n",
    "        # Different statistical tests are appropriate for normal vs non-normal data:\n",
    "        # - Normal data: t-test (more powerful, assumes normal distribution)\n",
    "        # - Non-normal data: Mann-Whitney U (robust, no distribution assumptions)\n",
    "        \n",
    "        # Data aggregation parameters:\n",
    "        self.aggregation_method = 'mean'            # How to aggregate features across nights ('mean' or 'median')\n",
    "        \n",
    "        # WHY USE MEAN AGGREGATION:\n",
    "        # Taking the mean across all nights for each participant gives us a stable\n",
    "        # estimate of their typical movement patterns, reducing night-to-night variability\n",
    "        # while preserving individual differences.\n",
    "        \n",
    "        # Initialize the supporting systems\n",
    "        self.setup_logging()                # Set up the system to record what happens\n",
    "        self.initialize_stats()             # Set up counters to track our progress\n",
    "        \n",
    "        # Print information about the analysis configuration\n",
    "        print(f\"Statistical Analysis Pipeline Initialized\")\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        print(f\"Feature dimension: {self.feature_dim}\")\n",
    "        print(f\"Bonferroni corrected alpha: {self.bonferroni_alpha:.2e}\")\n",
    "        print(f\"Effect size threshold: {self.medium_effect} (medium)\")\n",
    "        print(f\"Aggregation method: {self.aggregation_method}\")\n",
    "    \n",
    "    def initialize_stats(self):\n",
    "        \"\"\"\n",
    "        Set up counters to keep track of analysis statistics.\n",
    "        This helps us monitor progress and results throughout the analysis.\n",
    "        \"\"\"\n",
    "        self.stats = {\n",
    "            # Dataset information\n",
    "            'total_participants': 0,        # How many participants we analyzed\n",
    "            'controls_count': 0,            # Number of control participants\n",
    "            'irbd_count': 0,                # Number of iRBD participants\n",
    "            'total_nights': 0,              # Total nights across all participants\n",
    "            'total_windows': 0,             # Total 10-second windows processed\n",
    "            \n",
    "            # Feature analysis results\n",
    "            'total_features_tested': 0,     # Number of features tested (should be 1024)\n",
    "            'normal_features': 0,           # Number of features with normal distribution\n",
    "            'non_normal_features': 0,       # Number of features with non-normal distribution\n",
    "            'significant_features': 0,      # Number of statistically significant features\n",
    "            'large_effect_features': 0,     # Number of features with large effect sizes\n",
    "            'both_sig_and_large': 0,        # Features that are both significant AND have large effects\n",
    "            \n",
    "            # Analysis performance\n",
    "            'analysis_time': 0.0,           # Total analysis time in seconds\n",
    "            'memory_usage': 0.0,            # Peak memory usage during analysis\n",
    "            \n",
    "            # Top results\n",
    "            'top_features': [],             # Most important features identified\n",
    "            'effect_size_distribution': [], # Distribution of effect sizes across features\n",
    "            'p_value_distribution': []      # Distribution of p-values across features\n",
    "        }\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"\n",
    "        Set up the logging system to record everything that happens during analysis.\n",
    "        This creates a detailed record of the process for debugging and documentation.\n",
    "        \"\"\"\n",
    "        # Create a unique log file name with current date and time\n",
    "        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_file = self.log_dir / f\"statistical_analysis_{current_time}.log\"\n",
    "        \n",
    "        # Configure the logging system to write to both file and console\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,                     # Log all INFO level messages and above\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',  # Include timestamp and level\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),      # Save messages to log file\n",
    "                logging.StreamHandler(sys.stdout)   # Also display on screen\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create our logger object\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Write initial log messages to document the start of analysis\n",
    "        self.logger.info(f\"=== Statistical Analysis Pipeline Started ===\")\n",
    "        self.logger.info(f\"Base directory: {self.base_dir}\")\n",
    "        self.logger.info(f\"Feature dimension: {self.feature_dim}\")\n",
    "        self.logger.info(f\"Bonferroni corrected alpha: {self.bonferroni_alpha:.2e}\")\n",
    "        self.logger.info(f\"Effect size threshold: {self.medium_effect}\")\n",
    "        self.logger.info(f\"Aggregation method: {self.aggregation_method}\")\n",
    "    \n",
    "    def load_and_aggregate_features(self):\n",
    "        \"\"\"\n",
    "        Load feature data for all participants and calculate participant-level averages.\n",
    "        \n",
    "        WHAT THIS FUNCTION DOES:\n",
    "        1. Loads individual participant feature files from both groups\n",
    "        2. For each participant, calculates the average across all nights and windows\n",
    "        3. Creates two arrays: one for controls, one for iRBD patients\n",
    "        4. Each participant contributes one 1024-dimensional feature vector\n",
    "        \n",
    "        WHY AGGREGATE ACROSS NIGHTS:\n",
    "        - Reduces night-to-night variability within participants\n",
    "        - Creates stable estimates of individual movement patterns\n",
    "        - Enables participant-level statistical comparisons\n",
    "        - Simplifies the statistical analysis (one value per participant per feature)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (controls_features, irbd_features, participant_info)\n",
    "                - controls_features: Array of shape (n_controls, 1024)\n",
    "                - irbd_features: Array of shape (n_irbd, 1024)\n",
    "                - participant_info: Dictionary with participant details\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"ðŸ“– Loading and aggregating feature data...\")\n",
    "            \n",
    "            controls_features = []\n",
    "            irbd_features = []\n",
    "            participant_info = {\n",
    "                'controls': [],\n",
    "                'irbd': [],\n",
    "                'controls_nights': [],\n",
    "                'irbd_nights': []\n",
    "            }\n",
    "            \n",
    "            # Load control participants\n",
    "            self.logger.info(\"   Processing control participant features...\")\n",
    "            control_files = list(self.features_controls_dir.glob(\"*_features.npy\"))\n",
    "            \n",
    "            if len(control_files) == 0:\n",
    "                raise FileNotFoundError(f\"No control feature files found in {self.features_controls_dir}\")\n",
    "            \n",
    "            for file_path in sorted(control_files):\n",
    "                try:\n",
    "                    # Load participant data\n",
    "                    # Each file contains: {'features': array, 'windows_mask': array, 'participant_id': str}\n",
    "                    data = np.load(file_path, allow_pickle=True).item()\n",
    "                    \n",
    "                    # Extract the components\n",
    "                    features = data['features']  # Shape: (nights, max_windows_per_night, 1024)\n",
    "                    windows_mask = data['windows_mask']  # Shape: (nights, max_windows_per_night)\n",
    "                    participant_id = data['participant_id']\n",
    "                    \n",
    "                    # Calculate participant-level feature averages\n",
    "                    # We need to aggregate across all valid windows for this participant\n",
    "                    valid_features = []\n",
    "                    \n",
    "                    # Process each night separately\n",
    "                    for night_idx in range(features.shape[0]):\n",
    "                        night_features = features[night_idx]  # Shape: (max_windows_per_night, 1024)\n",
    "                        night_mask = windows_mask[night_idx]  # Shape: (max_windows_per_night,)\n",
    "                        \n",
    "                        # Get only the valid windows for this night (where mask is True)\n",
    "                        valid_night_features = night_features[night_mask]  # Shape: (valid_windows, 1024)\n",
    "                        \n",
    "                        # Only include nights with at least some valid data\n",
    "                        if len(valid_night_features) > 0:\n",
    "                            valid_features.append(valid_night_features)\n",
    "                    \n",
    "                    # If we have valid data for this participant\n",
    "                    if valid_features:\n",
    "                        # Concatenate all valid features across all nights\n",
    "                        all_features = np.concatenate(valid_features, axis=0)  # Shape: (total_valid_windows, 1024)\n",
    "                        \n",
    "                        # Calculate the mean across all windows (aggregation step)\n",
    "                        if self.aggregation_method == 'mean':\n",
    "                            participant_avg_features = np.mean(all_features, axis=0)  # Shape: (1024,)\n",
    "                        elif self.aggregation_method == 'median':\n",
    "                            participant_avg_features = np.median(all_features, axis=0)  # Shape: (1024,)\n",
    "                        \n",
    "                        # Store the results\n",
    "                        controls_features.append(participant_avg_features)\n",
    "                        participant_info['controls'].append(participant_id)\n",
    "                        participant_info['controls_nights'].append(len(valid_features))\n",
    "                        \n",
    "                        self.logger.info(f\"     - {participant_id}: {len(valid_features)} nights, \"\n",
    "                                       f\"{len(all_features)} total windows\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"     - Error loading {file_path.name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Load iRBD participants\n",
    "            self.logger.info(\"   Processing iRBD participant features...\")\n",
    "            irbd_files = list(self.features_irbd_dir.glob(\"*_features.npy\"))\n",
    "            \n",
    "            if len(irbd_files) == 0:\n",
    "                raise FileNotFoundError(f\"No iRBD feature files found in {self.features_irbd_dir}\")\n",
    "            \n",
    "            for file_path in sorted(irbd_files):\n",
    "                try:\n",
    "                    # Load participant data (same structure as controls)\n",
    "                    data = np.load(file_path, allow_pickle=True).item()\n",
    "                    \n",
    "                    # Extract the components\n",
    "                    features = data['features']  # Shape: (nights, max_windows_per_night, 1024)\n",
    "                    windows_mask = data['windows_mask']  # Shape: (nights, max_windows_per_night)\n",
    "                    participant_id = data['participant_id']\n",
    "                    \n",
    "                    # Calculate participant-level feature averages (same process as controls)\n",
    "                    valid_features = []\n",
    "                    \n",
    "                    for night_idx in range(features.shape[0]):\n",
    "                        night_features = features[night_idx]  # Shape: (max_windows_per_night, 1024)\n",
    "                        night_mask = windows_mask[night_idx]  # Shape: (max_windows_per_night,)\n",
    "                        \n",
    "                        # Get only the valid windows for this night\n",
    "                        valid_night_features = night_features[night_mask]  # Shape: (valid_windows, 1024)\n",
    "                        \n",
    "                        if len(valid_night_features) > 0:\n",
    "                            valid_features.append(valid_night_features)\n",
    "                    \n",
    "                    if valid_features:\n",
    "                        # Concatenate and aggregate\n",
    "                        all_features = np.concatenate(valid_features, axis=0)  # Shape: (total_valid_windows, 1024)\n",
    "                        \n",
    "                        if self.aggregation_method == 'mean':\n",
    "                            participant_avg_features = np.mean(all_features, axis=0)  # Shape: (1024,)\n",
    "                        elif self.aggregation_method == 'median':\n",
    "                            participant_avg_features = np.median(all_features, axis=0)  # Shape: (1024,)\n",
    "                        \n",
    "                        # Store the results\n",
    "                        irbd_features.append(participant_avg_features)\n",
    "                        participant_info['irbd'].append(participant_id)\n",
    "                        participant_info['irbd_nights'].append(len(valid_features))\n",
    "                        \n",
    "                        self.logger.info(f\"     - {participant_id}: {len(valid_features)} nights, \"\n",
    "                                       f\"{len(all_features)} total windows\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"     - Error loading {file_path.name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Convert lists to numpy arrays\n",
    "            controls_features = np.array(controls_features)  # Shape: (n_controls, 1024)\n",
    "            irbd_features = np.array(irbd_features)          # Shape: (n_irbd, 1024)\n",
    "            \n",
    "            # Check that we have sufficient data for analysis\n",
    "            if len(controls_features) < self.min_sample_size:\n",
    "                raise ValueError(f\"Insufficient control participants: {len(controls_features)} < {self.min_sample_size}\")\n",
    "            \n",
    "            if len(irbd_features) < self.min_sample_size:\n",
    "                raise ValueError(f\"Insufficient iRBD participants: {len(irbd_features)} < {self.min_sample_size}\")\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['total_participants'] = len(controls_features) + len(irbd_features)\n",
    "            self.stats['controls_count'] = len(controls_features)\n",
    "            self.stats['irbd_count'] = len(irbd_features)\n",
    "            self.stats['total_nights'] = (sum(participant_info['controls_nights']) + \n",
    "                                        sum(participant_info['irbd_nights']))\n",
    "            \n",
    "            # Log summary of loaded data\n",
    "            self.logger.info(f\"Feature data loaded and aggregated successfully:\")\n",
    "            self.logger.info(f\"   - Controls: {len(controls_features)} participants\")\n",
    "            self.logger.info(f\"   - iRBD: {len(irbd_features)} participants\")\n",
    "            self.logger.info(f\"   - Total nights: {self.stats['total_nights']}\")\n",
    "            self.logger.info(f\"   - Feature dimension: {controls_features.shape[1] if len(controls_features) > 0 else 0}\")\n",
    "            self.logger.info(f\"   - Aggregation method: {self.aggregation_method}\")\n",
    "            \n",
    "            return controls_features, irbd_features, participant_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading feature data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def perform_statistical_tests(self, controls_features, irbd_features):\n",
    "        \"\"\"\n",
    "        Perform comprehensive statistical analysis comparing features between groups.\n",
    "        \n",
    "        WHAT THIS FUNCTION DOES:\n",
    "        1. Tests each of the 1024 features for normality in both groups\n",
    "        2. Chooses appropriate statistical test based on normality results\n",
    "        3. Calculates effect sizes to quantify the magnitude of differences\n",
    "        4. Applies multiple comparison correction to control false discoveries\n",
    "        5. Identifies features with both statistical and practical significance\n",
    "        \n",
    "        STATISTICAL TESTS USED:\n",
    "        - Shapiro-Wilk test: Checks if data follows a normal distribution\n",
    "        - Independent t-test: Compares means when data is normally distributed\n",
    "        - Mann-Whitney U test: Compares distributions when data is not normal\n",
    "        - Bonferroni correction: Adjusts p-values for multiple comparisons\n",
    "        \n",
    "        EFFECT SIZE MEASURES:\n",
    "        - Cohen's d: For t-tests, measures standardized difference between means\n",
    "        - Rank-biserial correlation: For Mann-Whitney U, measures effect size\n",
    "        \n",
    "        Args:\n",
    "            controls_features: Feature matrix for control participants (n_controls, 1024)\n",
    "            irbd_features: Feature matrix for iRBD participants (n_irbd, 1024)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Comprehensive results for each feature including:\n",
    "                - Descriptive statistics for both groups\n",
    "                - Statistical test results and p-values\n",
    "                - Effect sizes and their interpretation\n",
    "                - Multiple comparison corrected results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Performing statistical tests for all features...\")\n",
    "            \n",
    "            # Check that we have enough data for meaningful analysis\n",
    "            if len(controls_features) < self.min_sample_size or len(irbd_features) < self.min_sample_size:\n",
    "                raise ValueError(f\"Insufficient sample size. Need at least {self.min_sample_size} \"\n",
    "                               f\"participants per group. Got {len(controls_features)} controls, \"\n",
    "                               f\"{len(irbd_features)} iRBD.\")\n",
    "            \n",
    "            # Initialize results storage\n",
    "            # We'll store results for each feature in these lists\n",
    "            feature_results = {\n",
    "                'feature_index': [],           # Which feature (0-1023)\n",
    "                'feature_name': [],            # Human-readable feature name\n",
    "                \n",
    "                # Descriptive statistics for controls\n",
    "                'control_mean': [],            # Average value in control group\n",
    "                'control_std': [],             # Standard deviation in control group\n",
    "                'control_median': [],          # Median value in control group\n",
    "                'control_iqr': [],             # Interquartile range in control group\n",
    "                \n",
    "                # Descriptive statistics for iRBD\n",
    "                'irbd_mean': [],               # Average value in iRBD group\n",
    "                'irbd_std': [],                # Standard deviation in iRBD group\n",
    "                'irbd_median': [],             # Median value in iRBD group\n",
    "                'irbd_iqr': [],                # Interquartile range in iRBD group\n",
    "                \n",
    "                # Group comparison\n",
    "                'mean_difference': [],         # Difference in means (iRBD - controls)\n",
    "                'median_difference': [],       # Difference in medians (iRBD - controls)\n",
    "                'percent_change': [],          # Percentage change from controls to iRBD\n",
    "                \n",
    "                # Normality testing\n",
    "                'control_normal': [],          # Is control group normally distributed?\n",
    "                'irbd_normal': [],             # Is iRBD group normally distributed?\n",
    "                'control_shapiro_p': [],       # P-value from Shapiro-Wilk test (controls)\n",
    "                'irbd_shapiro_p': [],          # P-value from Shapiro-Wilk test (iRBD)\n",
    "                \n",
    "                # Statistical testing\n",
    "                'test_type': [],               # Which test was used (t-test or Mann-Whitney U)\n",
    "                'test_statistic': [],          # Test statistic value\n",
    "                'p_value': [],                 # Raw p-value from statistical test\n",
    "                'p_value_corrected': [],       # Bonferroni corrected p-value\n",
    "                'is_significant': [],          # Is the corrected p-value < 0.05?\n",
    "                \n",
    "                # Effect size analysis\n",
    "                'effect_size': [],             # Effect size (Cohen's d or rank-biserial correlation)\n",
    "                'effect_size_interpretation': [], # Small/Medium/Large effect\n",
    "                'is_large_effect': [],         # Is effect size â‰¥ 0.5?\n",
    "                \n",
    "                # Combined significance\n",
    "                'significant_and_large': []    # Both statistically significant AND large effect?\n",
    "            }\n",
    "            \n",
    "            # Perform analysis for each feature\n",
    "            self.logger.info(f\"   Testing {self.feature_dim} features...\")\n",
    "            \n",
    "            for feature_idx in range(self.feature_dim):\n",
    "                # Extract feature values for both groups\n",
    "                control_values = controls_features[:, feature_idx]  # All control participants, this feature\n",
    "                irbd_values = irbd_features[:, feature_idx]         # All iRBD participants, this feature\n",
    "                \n",
    "                # Calculate descriptive statistics for controls\n",
    "                control_mean = np.mean(control_values)\n",
    "                control_std = np.std(control_values, ddof=1)  # Sample standard deviation (ddof=1)\n",
    "                control_median = np.median(control_values)\n",
    "                control_q75, control_q25 = np.percentile(control_values, [75, 25])\n",
    "                control_iqr = control_q75 - control_q25\n",
    "                \n",
    "                # Calculate descriptive statistics for iRBD\n",
    "                irbd_mean = np.mean(irbd_values)\n",
    "                irbd_std = np.std(irbd_values, ddof=1)  # Sample standard deviation\n",
    "                irbd_median = np.median(irbd_values)\n",
    "                irbd_q75, irbd_q25 = np.percentile(irbd_values, [75, 25])\n",
    "                irbd_iqr = irbd_q75 - irbd_q25\n",
    "                \n",
    "                # Calculate group differences\n",
    "                mean_difference = irbd_mean - control_mean\n",
    "                median_difference = irbd_median - control_median\n",
    "                \n",
    "                # Calculate percentage change (avoid division by zero)\n",
    "                if abs(control_mean) > 1e-10:  # If control mean is not essentially zero\n",
    "                    percent_change = (mean_difference / control_mean) * 100\n",
    "                else:\n",
    "                    percent_change = 0.0\n",
    "                \n",
    "                # Test for normality in both groups\n",
    "                # We use Shapiro-Wilk test if sample size is appropriate (3-5000)\n",
    "                control_normal = True\n",
    "                irbd_normal = True\n",
    "                control_shapiro_p = 1.0\n",
    "                irbd_shapiro_p = 1.0\n",
    "                \n",
    "                # Test normality for controls\n",
    "                if len(control_values) >= 3 and len(control_values) <= 5000:\n",
    "                    try:\n",
    "                        _, control_shapiro_p = shapiro(control_values)\n",
    "                        control_normal = control_shapiro_p > self.normality_alpha\n",
    "                    except:\n",
    "                        # If Shapiro-Wilk fails, assume non-normal\n",
    "                        control_normal = False\n",
    "                        control_shapiro_p = 0.0\n",
    "                \n",
    "                # Test normality for iRBD\n",
    "                if len(irbd_values) >= 3 and len(irbd_values) <= 5000:\n",
    "                    try:\n",
    "                        _, irbd_shapiro_p = shapiro(irbd_values)\n",
    "                        irbd_normal = irbd_shapiro_p > self.normality_alpha\n",
    "                    except:\n",
    "                        # If Shapiro-Wilk fails, assume non-normal\n",
    "                        irbd_normal = False\n",
    "                        irbd_shapiro_p = 0.0\n",
    "                \n",
    "                # Choose appropriate statistical test based on normality\n",
    "                if control_normal and irbd_normal:\n",
    "                    # Both groups are normal - use independent t-test\n",
    "                    # T-test compares the means of two groups\n",
    "                    test_stat, p_value = ttest_ind(control_values, irbd_values, equal_var=False)\n",
    "                    test_type = 'Independent t-test'\n",
    "                    \n",
    "                    # Calculate Cohen's d for effect size\n",
    "                    # Cohen's d measures the standardized difference between two means\n",
    "                    pooled_std = np.sqrt(((len(control_values) - 1) * control_std**2 + \n",
    "                                         (len(irbd_values) - 1) * irbd_std**2) / \n",
    "                                        (len(control_values) + len(irbd_values) - 2))\n",
    "                    \n",
    "                    if pooled_std > 0:\n",
    "                        effect_size = mean_difference / pooled_std\n",
    "                    else:\n",
    "                        effect_size = 0.0\n",
    "                    \n",
    "                else:\n",
    "                    # At least one group is not normal - use Mann-Whitney U test\n",
    "                    # Mann-Whitney U test compares the distributions of two groups\n",
    "                    # It's more robust and doesn't assume normal distributions\n",
    "                    test_stat, p_value = mannwhitneyu(control_values, irbd_values, \n",
    "                                                     alternative='two-sided')\n",
    "                    test_type = 'Mann-Whitney U'\n",
    "                    \n",
    "                    # Calculate rank-biserial correlation for effect size\n",
    "                    # This is the appropriate effect size measure for Mann-Whitney U test\n",
    "                    n1, n2 = len(control_values), len(irbd_values)\n",
    "                    if (n1 * n2) > 0:\n",
    "                        # Rank-biserial correlation formula\n",
    "                        effect_size = (test_stat - (n1 * n2) / 2) / (n1 * n2)\n",
    "                    else:\n",
    "                        effect_size = 0.0\n",
    "                \n",
    "                # Interpret effect size magnitude\n",
    "                abs_effect_size = abs(effect_size)\n",
    "                if abs_effect_size < self.small_effect:\n",
    "                    effect_interpretation = 'Negligible'\n",
    "                elif abs_effect_size < self.medium_effect:\n",
    "                    effect_interpretation = 'Small'\n",
    "                elif abs_effect_size < self.large_effect:\n",
    "                    effect_interpretation = 'Medium'\n",
    "                else:\n",
    "                    effect_interpretation = 'Large'\n",
    "                \n",
    "                # Determine if effect size is large enough to be practically significant\n",
    "                is_large_effect = abs_effect_size >= self.medium_effect\n",
    "                \n",
    "                # Store all results for this feature\n",
    "                feature_results['feature_index'].append(feature_idx)\n",
    "                feature_results['feature_name'].append(f\"Feature_{feature_idx:04d}\")\n",
    "                \n",
    "                # Descriptive statistics\n",
    "                feature_results['control_mean'].append(control_mean)\n",
    "                feature_results['control_std'].append(control_std)\n",
    "                feature_results['control_median'].append(control_median)\n",
    "                feature_results['control_iqr'].append(control_iqr)\n",
    "                feature_results['irbd_mean'].append(irbd_mean)\n",
    "                feature_results['irbd_std'].append(irbd_std)\n",
    "                feature_results['irbd_median'].append(irbd_median)\n",
    "                feature_results['irbd_iqr'].append(irbd_iqr)\n",
    "                \n",
    "                # Group comparisons\n",
    "                feature_results['mean_difference'].append(mean_difference)\n",
    "                feature_results['median_difference'].append(median_difference)\n",
    "                feature_results['percent_change'].append(percent_change)\n",
    "                \n",
    "                # Normality testing\n",
    "                feature_results['control_normal'].append(control_normal)\n",
    "                feature_results['irbd_normal'].append(irbd_normal)\n",
    "                feature_results['control_shapiro_p'].append(control_shapiro_p)\n",
    "                feature_results['irbd_shapiro_p'].append(irbd_shapiro_p)\n",
    "                \n",
    "                # Statistical testing\n",
    "                feature_results['test_type'].append(test_type)\n",
    "                feature_results['test_statistic'].append(test_stat)\n",
    "                feature_results['p_value'].append(p_value)\n",
    "                feature_results['effect_size'].append(effect_size)\n",
    "                feature_results['effect_size_interpretation'].append(effect_interpretation)\n",
    "                feature_results['is_large_effect'].append(is_large_effect)\n",
    "                \n",
    "                # Progress logging every 100 features\n",
    "                if (feature_idx + 1) % 100 == 0:\n",
    "                    self.logger.info(f\"     - Processed {feature_idx + 1}/{self.feature_dim} features\")\n",
    "            \n",
    "            # Apply multiple comparison correction (Bonferroni method)\n",
    "            self.logger.info(\"   Applying Bonferroni correction for multiple comparisons...\")\n",
    "            \n",
    "            # Extract all p-values for correction\n",
    "            p_values = np.array(feature_results['p_value'])\n",
    "            \n",
    "            # Apply Bonferroni correction\n",
    "            # This adjusts p-values to control the family-wise error rate\n",
    "            rejected, p_corrected, _, _ = multipletests(p_values, alpha=self.alpha, method='bonferroni')\n",
    "            \n",
    "            # Add corrected results to our data\n",
    "            feature_results['p_value_corrected'] = p_corrected.tolist()\n",
    "            feature_results['is_significant'] = rejected.tolist()\n",
    "            \n",
    "            # Identify features with both significance and large effect\n",
    "            both_sig_and_large = [sig and large for sig, large in \n",
    "                                 zip(feature_results['is_significant'], \n",
    "                                     feature_results['is_large_effect'])]\n",
    "            feature_results['significant_and_large'] = both_sig_and_large\n",
    "            \n",
    "            # Calculate summary statistics\n",
    "            n_significant = np.sum(rejected)\n",
    "            n_large_effect = np.sum(feature_results['is_large_effect'])\n",
    "            n_both = np.sum(both_sig_and_large)\n",
    "            n_normal = np.sum([c and i for c, i in zip(feature_results['control_normal'], \n",
    "                                                      feature_results['irbd_normal'])])\n",
    "            n_non_normal = self.feature_dim - n_normal\n",
    "            \n",
    "            # Update global statistics\n",
    "            self.stats['total_features_tested'] = self.feature_dim\n",
    "            self.stats['normal_features'] = n_normal\n",
    "            self.stats['non_normal_features'] = n_non_normal\n",
    "            self.stats['significant_features'] = n_significant\n",
    "            self.stats['large_effect_features'] = n_large_effect\n",
    "            self.stats['both_sig_and_large'] = n_both\n",
    "            self.stats['effect_size_distribution'] = feature_results['effect_size'].copy()\n",
    "            self.stats['p_value_distribution'] = feature_results['p_value'].copy()\n",
    "            \n",
    "            # Log summary results\n",
    "            self.logger.info(f\" Statistical testing completed:\")\n",
    "            self.logger.info(f\"   - Total features tested: {self.feature_dim}\")\n",
    "            self.logger.info(f\"   - Normal distributions: {n_normal} features\")\n",
    "            self.logger.info(f\"   - Non-normal distributions: {n_non_normal} features\")\n",
    "            self.logger.info(f\"   - Significant features (Bonferroni): {n_significant}\")\n",
    "            self.logger.info(f\"   - Large effect features (|d|â‰¥{self.medium_effect}): {n_large_effect}\")\n",
    "            self.logger.info(f\"   - Both significant AND large effect: {n_both}\")\n",
    "            \n",
    "            # Convert to DataFrame for easier handling and analysis\n",
    "            results_df = pd.DataFrame(feature_results)\n",
    "            \n",
    "            # Sort by effect size (descending) to identify most important features\n",
    "            results_df = results_df.sort_values('effect_size', key=abs, ascending=False)\n",
    "            \n",
    "            # Store top features for summary\n",
    "            top_features = results_df.head(20)  # Top 20 features by effect size\n",
    "            self.stats['top_features'] = top_features[['feature_index', 'effect_size', \n",
    "                                                      'p_value_corrected', 'test_type']].to_dict('records')\n",
    "            \n",
    "            return results_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in statistical testing: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_analysis_results(self, results_df, controls_features, irbd_features, participant_info):\n",
    "        \"\"\"\n",
    "        Save all analysis results to files for later use and documentation.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with statistical analysis results\n",
    "            controls_features: Control group feature matrix\n",
    "            irbd_features: iRBD group feature matrix\n",
    "            participant_info: Dictionary with participant information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Saving analysis results...\")\n",
    "            \n",
    "            # Save detailed statistical results\n",
    "            detailed_results_path = self.stats_results_dir / \"detailed_statistical_results.csv\"\n",
    "            results_df.to_csv(detailed_results_path, index=False)\n",
    "            self.logger.info(f\"   - Detailed results saved: {detailed_results_path}\")\n",
    "            \n",
    "            # Save significant features only\n",
    "            significant_features = results_df[results_df['is_significant']]\n",
    "            if len(significant_features) > 0:\n",
    "                sig_results_path = self.stats_results_dir / \"significant_features.csv\"\n",
    "                significant_features.to_csv(sig_results_path, index=False)\n",
    "                self.logger.info(f\"   - Significant features saved: {sig_results_path}\")\n",
    "            \n",
    "            # Save features with large effects\n",
    "            large_effect_features = results_df[results_df['is_large_effect']]\n",
    "            if len(large_effect_features) > 0:\n",
    "                large_effect_path = self.stats_results_dir / \"large_effect_features.csv\"\n",
    "                large_effect_features.to_csv(large_effect_path, index=False)\n",
    "                self.logger.info(f\"   - Large effect features saved: {large_effect_path}\")\n",
    "            \n",
    "            # Save features that are both significant and have large effects\n",
    "            both_features = results_df[results_df['significant_and_large']]\n",
    "            if len(both_features) > 0:\n",
    "                both_path = self.stats_results_dir / \"significant_and_large_effect_features.csv\"\n",
    "                both_features.to_csv(both_path, index=False)\n",
    "                self.logger.info(f\"   - Significant + large effect features saved: {both_path}\")\n",
    "            \n",
    "            # Save comprehensive analysis summary\n",
    "            summary = {\n",
    "                'analysis_date': datetime.now().isoformat(),\n",
    "                'dataset_info': {\n",
    "                    'total_participants': self.stats['total_participants'],\n",
    "                    'controls_count': self.stats['controls_count'],\n",
    "                    'irbd_count': self.stats['irbd_count'],\n",
    "                    'total_nights': self.stats['total_nights'],\n",
    "                    'aggregation_method': self.aggregation_method\n",
    "                },\n",
    "                'statistical_parameters': {\n",
    "                    'alpha': self.alpha,\n",
    "                    'bonferroni_alpha': self.bonferroni_alpha,\n",
    "                    'effect_size_threshold': self.medium_effect,\n",
    "                    'normality_alpha': self.normality_alpha,\n",
    "                    'min_sample_size': self.min_sample_size\n",
    "                },\n",
    "                'results_summary': {\n",
    "                    'total_features_tested': self.stats['total_features_tested'],\n",
    "                    'normal_features': self.stats['normal_features'],\n",
    "                    'non_normal_features': self.stats['non_normal_features'],\n",
    "                    'significant_features': self.stats['significant_features'],\n",
    "                    'large_effect_features': self.stats['large_effect_features'],\n",
    "                    'both_sig_and_large': self.stats['both_sig_and_large'],\n",
    "                    'significance_rate': self.stats['significant_features'] / self.stats['total_features_tested'] * 100,\n",
    "                    'large_effect_rate': self.stats['large_effect_features'] / self.stats['total_features_tested'] * 100\n",
    "                },\n",
    "                'top_features': self.stats['top_features'][:10],  # Top 10 features\n",
    "                'participant_info': participant_info\n",
    "            }\n",
    "            \n",
    "            def convert_numpy_types(obj):\n",
    "                \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "                if isinstance(obj, dict):\n",
    "                    return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [convert_numpy_types(item) for item in obj]\n",
    "                elif isinstance(obj, np.integer):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.floating):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                else:\n",
    "                    return obj\n",
    "\n",
    "            summary_path = self.stats_results_dir / \"analysis_summary.json\"\n",
    "            with open(summary_path, 'w') as f:\n",
    "                json.dump(convert_numpy_types(summary), f, indent=2)\n",
    "            \n",
    "            # Save raw feature matrices for potential future analysis\n",
    "            raw_data_path = self.stats_results_dir / \"raw_feature_matrices.npz\"\n",
    "            np.savez_compressed(raw_data_path,\n",
    "                              controls_features=controls_features,\n",
    "                              irbd_features=irbd_features,\n",
    "                              controls_ids=participant_info['controls'],\n",
    "                              irbd_ids=participant_info['irbd'])\n",
    "            self.logger.info(f\"   - Raw feature matrices saved: {raw_data_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving analysis results: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_comprehensive_visualizations(self, results_df, controls_features, irbd_features):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for the statistical analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with statistical analysis results\n",
    "            controls_features: Control group feature matrix\n",
    "            irbd_features: iRBD group feature matrix\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Creating comprehensive visualizations...\")\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            self.plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 1. Volcano plot: Effect size vs Statistical significance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Extract data for plotting\n",
    "            effect_sizes = results_df['effect_size'].values\n",
    "            p_values_log = -np.log10(results_df['p_value_corrected'].values + 1e-300)  # Add small value to avoid log(0)\n",
    "            \n",
    "            # Color points based on significance and effect size\n",
    "            colors = []\n",
    "            for _, row in results_df.iterrows():\n",
    "                if row['significant_and_large']:\n",
    "                    colors.append('red')  # Both significant and large effect\n",
    "                elif row['is_significant']:\n",
    "                    colors.append('orange')  # Significant only\n",
    "                elif row['is_large_effect']:\n",
    "                    colors.append('blue')  # Large effect only\n",
    "                else:\n",
    "                    colors.append('lightgray')  # Neither\n",
    "            \n",
    "            # Create scatter plot\n",
    "            plt.scatter(effect_sizes, p_values_log, c=colors, alpha=0.6, s=30)\n",
    "            \n",
    "            # Add significance and effect size thresholds\n",
    "            plt.axhline(y=-np.log10(self.bonferroni_alpha), color='red', linestyle='--', \n",
    "                       label=f'Bonferroni threshold (Î±={self.bonferroni_alpha:.2e})')\n",
    "            plt.axvline(x=self.medium_effect, color='blue', linestyle='--', \n",
    "                       label=f'Effect size threshold (|d|={self.medium_effect})')\n",
    "            plt.axvline(x=-self.medium_effect, color='blue', linestyle='--')\n",
    "            \n",
    "            plt.xlabel('Effect Size (Cohen\\'s d or rank-biserial correlation)', fontsize=12)\n",
    "            plt.ylabel('-logâ‚â‚€(Corrected p-value)', fontsize=12)\n",
    "            plt.title('Feature Significance Analysis - iRBD vs Controls\\nVolcano Plot', fontsize=14, fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add text annotations for quadrants\n",
    "            plt.text(0.7, max(p_values_log)*0.9, 'High Effect\\nSignificant', \n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "            plt.text(-0.7, max(p_values_log)*0.9, 'High Effect\\nSignificant', \n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "            \n",
    "            volcano_path = self.plots_dir / \"feature_volcano_plot.png\"\n",
    "            plt.savefig(volcano_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.info(f\"   - Volcano plot saved: {volcano_path}\")\n",
    "            \n",
    "            # 2. Top discriminative features\n",
    "            top_features = results_df.head(20)  # Top 20 by effect size\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            y_pos = np.arange(len(top_features))\n",
    "            \n",
    "            # Create horizontal bar plot\n",
    "            colors = ['red' if both else 'orange' if sig else 'lightblue' \n",
    "                     for both, sig in zip(top_features['significant_and_large'], \n",
    "                                         top_features['is_significant'])]\n",
    "            \n",
    "            bars = plt.barh(y_pos, top_features['effect_size'].abs(), color=colors)\n",
    "            \n",
    "            plt.yticks(y_pos, [f'Feature {idx}' for idx in top_features['feature_index']])\n",
    "            plt.xlabel('|Effect Size|', fontsize=12)\n",
    "            plt.title('Top 20 Most Discriminative Features', fontsize=14, fontweight='bold')\n",
    "            plt.axvline(x=self.medium_effect, color='blue', linestyle='--', \n",
    "                       label=f'Medium effect threshold (|d|={self.medium_effect})')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (bar, effect_size) in enumerate(zip(bars, top_features['effect_size'].abs())):\n",
    "                plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{effect_size:.3f}', va='center', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            top_features_path = self.plots_dir / \"top_discriminative_features.png\"\n",
    "            plt.savefig(top_features_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.info(f\"   - Top features plot saved: {top_features_path}\")\n",
    "            \n",
    "            # 3. Statistical analysis summary dashboard\n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # Subplot 1: Feature significance summary\n",
    "            categories = ['Total\\nFeatures', 'Significant\\n(Bonferroni)', 'Large Effect\\n(|d|â‰¥0.5)', \n",
    "                         'Both Sig.\\n& Large']\n",
    "            counts = [\n",
    "                self.stats['total_features_tested'],\n",
    "                self.stats['significant_features'],\n",
    "                self.stats['large_effect_features'],\n",
    "                self.stats['both_sig_and_large']\n",
    "            ]\n",
    "            \n",
    "            bars1 = ax1.bar(categories, counts, color=['lightgray', 'orange', 'lightblue', 'red'])\n",
    "            ax1.set_title('Feature Analysis Summary', fontweight='bold')\n",
    "            ax1.set_ylabel('Number of Features')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, count in zip(bars1, counts):\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01, \n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Subplot 2: Effect size distribution\n",
    "            ax2.hist(results_df['effect_size'].abs(), bins=50, alpha=0.7, color='skyblue', \n",
    "                    edgecolor='black')\n",
    "            ax2.axvline(x=self.medium_effect, color='red', linestyle='--', \n",
    "                       label=f'Medium effect (|d|={self.medium_effect})')\n",
    "            ax2.set_title('Distribution of |Effect Sizes|', fontweight='bold')\n",
    "            ax2.set_xlabel('|Effect Size|')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Subplot 3: P-value distribution\n",
    "            ax3.hist(-np.log10(results_df['p_value'] + 1e-300), bins=50, alpha=0.7, \n",
    "                    color='lightcoral', edgecolor='black')\n",
    "            ax3.axvline(x=-np.log10(self.bonferroni_alpha), color='red', linestyle='--', \n",
    "                       label=f'Bonferroni threshold')\n",
    "            ax3.set_title('Distribution of -logâ‚â‚€(p-values)', fontweight='bold')\n",
    "            ax3.set_xlabel('-logâ‚â‚€(p-value)')\n",
    "            ax3.set_ylabel('Frequency')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Subplot 4: Test type usage\n",
    "            test_counts = results_df['test_type'].value_counts()\n",
    "            colors = ['lightgreen', 'lightcoral']\n",
    "            wedges, texts, autotexts = ax4.pie(test_counts.values, labels=test_counts.index, \n",
    "                                              autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "            ax4.set_title('Statistical Tests Used', fontweight='bold')\n",
    "            \n",
    "            plt.suptitle('Statistical Analysis Dashboard - iRBD vs Controls', fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            dashboard_path = self.plots_dir / \"statistical_analysis_dashboard.png\"\n",
    "            plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.info(f\"   - Analysis dashboard saved: {dashboard_path}\")\n",
    "            \n",
    "            # 4. Group comparison for top features\n",
    "            top_significant = results_df[results_df['significant_and_large']].head(6)\n",
    "            \n",
    "            if len(top_significant) > 0:\n",
    "                fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for i, (_, feature_row) in enumerate(top_significant.iterrows()):\n",
    "                    if i >= 6:\n",
    "                        break\n",
    "                    \n",
    "                    feature_idx = feature_row['feature_index']\n",
    "                    \n",
    "                    # Get feature values for both groups\n",
    "                    control_values = controls_features[:, feature_idx]\n",
    "                    irbd_values = irbd_features[:, feature_idx]\n",
    "                    \n",
    "                    # Create box plot\n",
    "                    data_to_plot = [control_values, irbd_values]\n",
    "                    box_plot = axes[i].boxplot(data_to_plot, labels=['Controls', 'iRBD'], \n",
    "                                              patch_artist=True)\n",
    "                    \n",
    "                    # Color the boxes\n",
    "                    box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "                    box_plot['boxes'][1].set_facecolor('lightcoral')\n",
    "                    \n",
    "                    axes[i].set_title(f'Feature {feature_idx}\\n'\n",
    "                                     f'Effect size: {feature_row[\"effect_size\"]:.3f}\\n'\n",
    "                                     f'p-value: {feature_row[\"p_value_corrected\"]:.2e}')\n",
    "                    axes[i].set_ylabel('Feature Value')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Hide unused subplots\n",
    "                for i in range(len(top_significant), 6):\n",
    "                    axes[i].set_visible(False)\n",
    "                \n",
    "                plt.suptitle('Top Discriminative Features - Group Comparisons', fontsize=16, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                group_comparison_path = self.plots_dir / \"top_features_group_comparison.png\"\n",
    "                plt.savefig(group_comparison_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                self.logger.info(f\"   - Group comparison plot saved: {group_comparison_path}\")\n",
    "            \n",
    "            # 5. Feature space visualization using t-SNE\n",
    "            if len(controls_features) > 0 and len(irbd_features) > 0:\n",
    "                self.logger.info(\"   Creating t-SNE visualization of feature space...\")\n",
    "                \n",
    "                # Combine all features\n",
    "                all_features = np.vstack([controls_features, irbd_features])\n",
    "                all_labels = ['Controls'] * len(controls_features) + ['iRBD'] * len(irbd_features)\n",
    "                \n",
    "                # Standardize features for t-SNE\n",
    "                scaler = StandardScaler()\n",
    "                features_scaled = scaler.fit_transform(all_features)\n",
    "                \n",
    "                # Apply t-SNE\n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_features)-1))\n",
    "                features_2d = tsne.fit_transform(features_scaled)\n",
    "                \n",
    "                # Create t-SNE plot\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                \n",
    "                # Plot controls\n",
    "                control_indices = np.array(all_labels) == 'Controls'\n",
    "                plt.scatter(features_2d[control_indices, 0], features_2d[control_indices, 1], \n",
    "                           c='blue', alpha=0.6, s=50, label='Controls')\n",
    "                \n",
    "                # Plot iRBD\n",
    "                irbd_indices = np.array(all_labels) == 'iRBD'\n",
    "                plt.scatter(features_2d[irbd_indices, 0], features_2d[irbd_indices, 1], \n",
    "                           c='red', alpha=0.6, s=50, label='iRBD')\n",
    "                \n",
    "                plt.xlabel('t-SNE Component 1')\n",
    "                plt.ylabel('t-SNE Component 2')\n",
    "                plt.title('t-SNE Visualization of Feature Space\\niRBD vs Controls', fontweight='bold')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                tsne_path = self.plots_dir / \"feature_space_tsne.png\"\n",
    "                plt.savefig(tsne_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                self.logger.info(f\"   - t-SNE visualization saved: {tsne_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_statistical_analysis(self):\n",
    "        \"\"\"\n",
    "        Main function to run the complete statistical analysis pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Running complete statistical analysis pipeline...\")\n",
    "            \n",
    "            # Record start time\n",
    "            analysis_start_time = datetime.now()\n",
    "            \n",
    "            # Load and aggregate feature data\n",
    "            controls_features, irbd_features, participant_info = self.load_and_aggregate_features()\n",
    "            \n",
    "            # Perform statistical tests\n",
    "            results_df = self.perform_statistical_tests(controls_features, irbd_features)\n",
    "            \n",
    "            # Save results\n",
    "            self.save_analysis_results(results_df, controls_features, irbd_features, participant_info)\n",
    "            \n",
    "            # Create visualizations\n",
    "            self.create_comprehensive_visualizations(results_df, controls_features, irbd_features)\n",
    "            \n",
    "            # Calculate analysis time\n",
    "            analysis_time = (datetime.now() - analysis_start_time).total_seconds()\n",
    "            self.stats['analysis_time'] = analysis_time\n",
    "            \n",
    "            # Show final results\n",
    "            self.print_final_statistics()\n",
    "            \n",
    "            self.logger.info(\"STATISTICAL ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"STATISTICAL ANALYSIS FAILED: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def print_final_statistics(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive summary of the statistical analysis results.\n",
    "        \"\"\"\n",
    "        # Print header\n",
    "        self.logger.info(f\"\\n{'='*70}\")\n",
    "        self.logger.info(f\"STATISTICAL ANALYSIS COMPLETED\")\n",
    "        self.logger.info(f\"{'='*70}\")\n",
    "        \n",
    "        # Dataset statistics\n",
    "        self.logger.info(f\"Dataset Statistics:\")\n",
    "        self.logger.info(f\"  - Total participants: {self.stats['total_participants']}\")\n",
    "        self.logger.info(f\"  - Controls: {self.stats['controls_count']}\")\n",
    "        self.logger.info(f\"  - iRBD: {self.stats['irbd_count']}\")\n",
    "        self.logger.info(f\"  - Total nights: {self.stats['total_nights']}\")\n",
    "        self.logger.info(f\"  - Aggregation method: {self.aggregation_method}\")\n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Statistical analysis results\n",
    "        self.logger.info(f\"Feature Analysis Results:\")\n",
    "        self.logger.info(f\"  - Total features tested: {self.stats['total_features_tested']}\")\n",
    "        self.logger.info(f\"  - Normal distributions: {self.stats['normal_features']}\")\n",
    "        self.logger.info(f\"  - Non-normal distributions: {self.stats['non_normal_features']}\")\n",
    "        self.logger.info(f\"  - Significant features (Bonferroni): {self.stats['significant_features']}\")\n",
    "        self.logger.info(f\"  - Large effect features (|d|â‰¥{self.medium_effect}): {self.stats['large_effect_features']}\")\n",
    "        self.logger.info(f\"  - Both significant AND large effect: {self.stats['both_sig_and_large']}\")\n",
    "        \n",
    "        if self.stats['total_features_tested'] > 0:\n",
    "            sig_rate = self.stats['significant_features'] / self.stats['total_features_tested'] * 100\n",
    "            effect_rate = self.stats['large_effect_features'] / self.stats['total_features_tested'] * 100\n",
    "            both_rate = self.stats['both_sig_and_large'] / self.stats['total_features_tested'] * 100\n",
    "            self.logger.info(f\"  - Significance rate: {sig_rate:.2f}%\")\n",
    "            self.logger.info(f\"  - Large effect rate: {effect_rate:.2f}%\")\n",
    "            self.logger.info(f\"  - Both significant & large effect rate: {both_rate:.2f}%\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Statistical parameters used\n",
    "        self.logger.info(f\"Statistical Parameters:\")\n",
    "        self.logger.info(f\"  - Significance level (Î±): {self.alpha}\")\n",
    "        self.logger.info(f\"  - Bonferroni corrected Î±: {self.bonferroni_alpha:.2e}\")\n",
    "        self.logger.info(f\"  - Effect size threshold: {self.medium_effect}\")\n",
    "        self.logger.info(f\"  - Normality test Î±: {self.normality_alpha}\")\n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Analysis performance\n",
    "        self.logger.info(f\"Analysis Performance:\")\n",
    "        self.logger.info(f\"  - Total analysis time: {self.stats['analysis_time']:.1f} seconds\")\n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Top discriminative features\n",
    "        if self.stats['top_features']:\n",
    "            self.logger.info(f\"Top 5 Most Discriminative Features:\")\n",
    "            for i, feature in enumerate(self.stats['top_features'][:5], 1):\n",
    "                self.logger.info(f\"  {i}. Feature {feature['feature_index']}: \"\n",
    "                               f\"|d|={abs(feature['effect_size']):.3f}, \"\n",
    "                               f\"p={feature['p_value_corrected']:.2e}, \"\n",
    "                               f\"test={feature['test_type']}\")\n",
    "            self.logger.info(\"\")\n",
    "        \n",
    "        # Clinical interpretation\n",
    "        self.logger.info(f\"Clinical Interpretation:\")\n",
    "        if self.stats['both_sig_and_large'] > 0:\n",
    "            self.logger.info(f\"  Found {self.stats['both_sig_and_large']} features with both statistical\")\n",
    "            self.logger.info(f\"     significance and large effect sizes\")\n",
    "            self.logger.info(f\"  These features represent movement patterns that reliably\")\n",
    "            self.logger.info(f\"     distinguish iRBD patients from healthy controls\")\n",
    "            self.logger.info(f\"  Results support the use of SSL-Wearables features for iRBD detection\")\n",
    "        else:\n",
    "            self.logger.info(f\"   No features found with both statistical significance and large effects\")\n",
    "            self.logger.info(f\"   May need larger sample size or different analysis approach\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Final success message\n",
    "        if self.stats['total_features_tested'] > 0:\n",
    "            self.logger.info(\"Statistical analysis pipeline completed successfully!\")\n",
    "            self.logger.info(\"Complete analysis ready for thesis writing and publication\")\n",
    "        else:\n",
    "            self.logger.error(\"Statistical analysis pipeline failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e2f94-08ae-49cb-b57a-4d864a0d0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that runs when the script is executed directly.\n",
    "    Creates a FeatureStatisticalAnalysis object and runs the entire analysis process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create and run the statistical analysis pipeline\n",
    "        pipeline = FeatureStatisticalAnalysis()\n",
    "        pipeline.run_statistical_analysis()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Statistical analysis interrupted by user\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Statistical analysis failed with error: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# SCRIPT EXECUTION ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
