{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION\n",
    "# Extract high-level movement features from preprocessed accelerometer data using the SSL-Wearables model.\n",
    "\n",
    "## INPUT\n",
    "# Source : .h5 files (from preprocessing stage)\n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/data/preprocessed/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/preprocessed/irbd/ \n",
    "# Format : Clean 30Hz accelerometer data segmented by nights\n",
    "\n",
    "\n",
    "## PIPELINE\n",
    "# 1. Night segmentation : Divide each night into 10-minute segments\n",
    "# 2. Window creation : Create 10-second non-overlapping windows within each segment\n",
    "# 3. Feature extraction : \n",
    "#    - Load SSL-Wearables harnet10 model via torch.hub.load()\n",
    "#    - Process windows in batches for GPU efficiency\n",
    "#    - Extract 1024-dimensional features using model.feature_extractor\n",
    "# 4. Quality control : Validate feature dimensions and handle edge cases\n",
    "# 5. Memory management : Clear GPU cache between participants\n",
    "# 6. Save features : Save per-participant features, then auto-combine for LSTM\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# Format : .npy files\n",
    "# Directories : \n",
    "#    - /work3/s184484/iRBD-detection/data/features/controls/\n",
    "#    - /work3/s184484/iRBD-detection/data/features/irbd/\n",
    "# Individual files : One .npy file per participant containing night-structured features\n",
    "# Structure within directories : \n",
    "# ├── features/\n",
    "# │   ├── controls/\n",
    "# │   │   ├── PARTICIPANTID1.npy     # Night-structured features: (nights, windows_per_night, 1024)\n",
    "# │   │   ├── PARTICIPANTID2.npy     # Night-structured features: (nights, windows_per_night, 1024)\n",
    "# │   │   └── ...\n",
    "# │   └── irbd/\n",
    "# │       ├── PARTICIPANTID1.npy     # Night-structured features: (nights, windows_per_night, 1024)\n",
    "# │       ├── PARTICIPANTID2.npy     # Night-structured features: (nights, windows_per_night, 1024)\n",
    "# │       └── ...\n",
    "# File Format Details:\n",
    "# Each .npy file contains a dictionary with:\n",
    "#    - 'features': numpy array with shape (nights, windows_per_night, 1024)\n",
    "#    - 'windows_mask': boolean mask indicating valid windows\n",
    "#    - 'participant_id': string identifier for the participant\n",
    "#    - 'num_nights': integer number of recording nights\n",
    "#    - 'total_windows': total number of windows across all nights\n",
    "#    - 'extraction_date': timestamp of when features were extracted\n",
    "#    - Other metadata for validation and tracking\n",
    "\n",
    "\n",
    "## VALIDATION\n",
    "# Feature dimension consistency : All outputs have exactly 1024 features\n",
    "# Window count validation : Expected number of windows per night (2880)\n",
    "# Data integrity : No NaN or infinite values in feature vectors\n",
    "# Model loading verification : SSL-Wearables model loads correctly\n",
    "# GPU utilization : Efficient memory usage and processing\n",
    "\n",
    "\n",
    "## PARAMETERS SUMMARY\n",
    "# Segment duration : 600 seconds (10 minutes)\n",
    "# Window size : 300 samples (10 seconds at 30Hz)\n",
    "# Windows per segment : 60 (non-overlapping)\n",
    "# Windows per night : 8 hours × 60 minutes × 10 seconds = 2880 windows per night\n",
    "# Windows per participant : Number of nights (of participant) × 2880\n",
    "# Feature dimension : 1024 (feature dimensions from SSL-Wearables ResNet output)\n",
    "# Model : 'harnet10' from 'OxWearables/ssl-wearables'\n",
    "# Batch size : 8-16 (depeding on GPU memory)\n",
    "# Input format : (batch, 3, 300) - channels first for SSL-Wearables\n",
    "# Pre-training : 700,000+ person-days of accelerometer data\n",
    "\n",
    "\n",
    "## ENVIRONMENT : env_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python libraries for file operations and system control\n",
    "import os                    # Operating System interface - helps us work with files and folders\n",
    "import sys                   # System-specific parameters - helps us control the program execution\n",
    "import h5py                  # HDF5 library - for reading the preprocessed .h5 files\n",
    "import numpy as np           # NumPy - for mathematical operations on arrays of numbers\n",
    "import pandas as pd          # Pandas - for working with data tables and organizing information\n",
    "from datetime import datetime, timedelta  # For working with dates and times\n",
    "import logging               # For creating detailed log files that record what the program does\n",
    "from pathlib import Path     # For easier and more reliable file path handling\n",
    "import glob                  # For finding files that match specific patterns (like all .h5 files)\n",
    "import traceback             # For showing detailed error messages when something goes wrong\n",
    "import json                  # For saving and loading JSON files (configuration and metadata)\n",
    "import gc                    # Garbage collection - for managing memory usage\n",
    "\n",
    "# Visualization libraries for creating plots and charts\n",
    "import matplotlib.pyplot as plt    # Main plotting library - like creating graphs in Excel\n",
    "import seaborn as sns             # Statistical plotting library - makes beautiful, professional plots\n",
    "\n",
    "# Configure matplotlib and seaborn for professional-looking plots\n",
    "plt.style.use('seaborn-v0_8')     # Use seaborn's visual style (makes plots look professional)\n",
    "sns.set_palette(\"husl\")           # Set a nice color palette (colors that work well together)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)  # Set default size for all plots (12 inches wide, 8 inches tall)\n",
    "plt.rcParams['font.size'] = 10    # Set default font size for all text in plots\n",
    "\n",
    "# Try to import PyTorch and SSL-Wearables - essential for feature extraction\n",
    "try:\n",
    "    import torch               # PyTorch - deep learning framework for running SSL-Wearables model\n",
    "    import torch.nn as nn      # Neural network components from PyTorch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check if CUDA (GPU) is available for faster processing\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available - will use CPU (slower)\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    # If PyTorch is not installed, show an error message and stop the program\n",
    "    print(f\"Error importing PyTorch: {e}\")\n",
    "    print(\"Please install PyTorch with: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Try to load SSL-Wearables model via torch.hub\n",
    "try:\n",
    "    print(\"Loading SSL-Wearables model...\")\n",
    "    # This downloads and loads the pre-trained harnet10 model\n",
    "    # SSL-Wearables is trained on over 700,000 days of accelerometer data\n",
    "    ssl_model = torch.hub.load('OxWearables/ssl-wearables', 'harnet10', pretrained=True, trust_repo=True)\n",
    "    print(\"SSL-Wearables model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SSL-Wearables model: {e}\")\n",
    "    print(\"Please check internet connection and torch.hub access\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE EXTRACTION PIPELINE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class FeatureExtractionPipeline:\n",
    "    \"\"\"\n",
    "    This class handles the extraction of SSL-Wearables features from preprocessed\n",
    "    accelerometer data. It organizes features by night (not concatenated) to preserve\n",
    "    the temporal structure needed for machine learning models.\n",
    "    \n",
    "    WHAT SSL-WEARABLES IS:\n",
    "    SSL-Wearables is a self-supervised learning model trained on over 700,000 days\n",
    "    of accelerometer data. It has learned to recognize a wide range of human\n",
    "    activities and movement patterns, making it perfect for our iRBD detection task.\n",
    "    \n",
    "    WHAT THIS CLASS DOES:\n",
    "    1. Reads preprocessed .h5 files containing night-segmented accelerometer data\n",
    "    2. Creates 10-second windows from each night's data separately\n",
    "    3. Processes windows through SSL-Wearables to extract 1024-dimensional features\n",
    "    4. Organizes features by night: (nights, windows_per_night, 1024_features)\n",
    "    5. Saves structured feature arrays suitable for machine learning models\n",
    "    \n",
    "    WHY NIGHT-BASED ORGANIZATION:\n",
    "    - Preserves temporal structure of sleep data\n",
    "    - Allows models to learn night-to-night patterns\n",
    "    - Enables proper cross-validation (participant-level splits)\n",
    "    - Supports variable-length sequences (different numbers of nights per participant)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the feature extraction pipeline with all necessary configuration.\n",
    "        This sets up directories, parameters, and the SSL-Wearables model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # =================================================================\n",
    "        # CONFIGURATION SECTION - Choose between example test and full processing\n",
    "        # =================================================================\n",
    "        \n",
    "        # FOR EXAMPLE FILE TESTING\n",
    "        self.base_dir = Path(\"/work3/s184484/iRBD-detection\")  # Main project folder on HPC\n",
    "        #self.mode = \"EXAMPLE_TEST\"                             # Tell the script we're testing with example\n",
    "        self.example_participant = \"2290025_90001_0_0\"        # Example participant ID (without .h5 extension)\n",
    "        \n",
    "        # FOR FULL DATASET PROCESSING \n",
    "        self.mode = \"FULL_PROCESSING\"                          # Tell the script we're processing all files\n",
    "        \n",
    "        # =================================================================\n",
    "        # DIRECTORY SETUP - Define where to find files and save results\n",
    "        # =================================================================\n",
    "        \n",
    "        # Input directories (where the preprocessed .h5 files are stored):\n",
    "        self.preprocessed_controls_dir = self.base_dir / \"data\" / \"preprocessed\" / \"controls\"  # Healthy people's data\n",
    "        self.preprocessed_irbd_dir = self.base_dir / \"data\" / \"preprocessed\" / \"irbd\"          # iRBD patients' data\n",
    "        \n",
    "        # Output directories (where to save the extracted features):\n",
    "        self.features_dir = self.base_dir / \"data\" / \"features\"                            # Main features directory\n",
    "        self.features_controls_dir = self.features_dir / \"controls\"                       # Individual control features\n",
    "        self.features_irbd_dir = self.features_dir / \"irbd\"                               # Individual iRBD features\n",
    "        self.features_combined_dir = self.features_dir / \"combined\"                       # Combined training datasets\n",
    "        \n",
    "        # Visualization output directory (where to save plots for the report):\n",
    "        self.plots_dir = self.base_dir / \"results\" / \"visualizations\"\n",
    "        self.example_plots_dir = self.plots_dir / \"example_testing\"  # Specific folder for example file plots\n",
    "        \n",
    "        # Only create log directory if it doesn't exist (for logging only)\n",
    "        self.log_dir = self.base_dir / \"validation\" / \"data_quality_reports\"\n",
    "        if not self.log_dir.exists():\n",
    "            self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # =================================================================\n",
    "        # PROCESSING PARAMETERS - Settings that control feature extraction\n",
    "        # =================================================================\n",
    "        \n",
    "        # Windowing parameters (how we split continuous data into chunks):\n",
    "        self.window_size_seconds = 10           # Each window is 10 seconds long\n",
    "        self.sampling_rate = 30                 # Data is sampled at 30Hz (30 samples per second)\n",
    "        self.window_size_samples = self.window_size_seconds * self.sampling_rate  # 300 samples per window\n",
    "        self.window_overlap = 0.0               # No overlap between windows (0% overlap)\n",
    "        \n",
    "        # Expected windows per night calculation:\n",
    "        # 8 hours × 60 minutes/hour × 60 seconds/minute = 28,800 seconds per night\n",
    "        # 28,800 seconds ÷ 10 seconds/window = 2,880 windows per perfect 8-hour night\n",
    "        self.expected_windows_per_night = 8 * 60 * 60 // self.window_size_seconds  # 2880 windows\n",
    "        \n",
    "        # WHY 10-SECOND WINDOWS:\n",
    "        # 10 seconds captures meaningful movement patterns while providing enough\n",
    "        # temporal resolution for detecting brief iRBD episodes during sleep.\n",
    "        \n",
    "        # Model parameters (settings for SSL-Wearables processing):\n",
    "        self.feature_dim = 1024                 # SSL-Wearables outputs 1024-dimensional feature vectors\n",
    "        self.batch_size = 512                   # Process 512 windows at once (GPU memory dependent)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "        \n",
    "        # WHY BATCH PROCESSING:\n",
    "        # Processing windows in batches is much faster than one-by-one processing\n",
    "        # and makes efficient use of GPU memory.\n",
    "        \n",
    "        # Visualization parameters (control which plots to create):\n",
    "        self.create_individual_plots = True     # Create detailed plots for each participant\n",
    "        self.create_summary_plots = True       # Create overall summary plots\n",
    "        \n",
    "        # =================================================================\n",
    "        # MODEL SETUP - Prepare SSL-Wearables for feature extraction\n",
    "        # =================================================================\n",
    "        \n",
    "        # Move the SSL-Wearables model to GPU (if available) for faster processing\n",
    "        self.ssl_model = ssl_model.to(self.device)\n",
    "        \n",
    "        # Set the model to evaluation mode (no training, just inference)\n",
    "        self.ssl_model.eval()\n",
    "        \n",
    "        # Disable gradient computation (saves memory and speeds up processing)\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        # Log model information\n",
    "        print(f\"SSL-Wearables model ready on device: {self.device}\")\n",
    "        print(f\"Feature dimension: {self.feature_dim}\")\n",
    "        print(f\"Window size: {self.window_size_seconds}s ({self.window_size_samples} samples)\")\n",
    "        print(f\"Expected windows per night: ~{self.expected_windows_per_night}\")\n",
    "        \n",
    "        # Initialize the supporting systems\n",
    "        self.setup_logging()                # Set up the system to record what happens\n",
    "        self.initialize_stats()             # Set up counters to track our progress\n",
    "        \n",
    "        # Print information about what mode we're running in\n",
    "        print(f\"Running in {self.mode} mode\")\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "    \n",
    "    def initialize_stats(self):\n",
    "        \"\"\"\n",
    "        Set up counters to keep track of processing statistics.\n",
    "        This helps us monitor success rates and identify any problems.\n",
    "        \"\"\"\n",
    "        self.stats = {\n",
    "            'total_participants': 0,        # How many participant files we found\n",
    "            'processed_participants': 0,    # How many participants we successfully processed\n",
    "            'failed_participants': 0,       # How many participants had errors\n",
    "            'total_nights': 0,              # Total number of nights across all participants\n",
    "            'total_windows': 0,             # Total number of 10-second windows processed\n",
    "            'total_features_extracted': 0,  # Total number of feature vectors created\n",
    "            'controls_processed': 0,        # How many control participants processed\n",
    "            'irbd_processed': 0,            # How many iRBD participants processed\n",
    "            'processing_time_total': 0.0    # Total time spent on feature extraction\n",
    "        }\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"\n",
    "        Set up the logging system to record everything that happens during feature extraction.\n",
    "        This creates a detailed record of the process for debugging and documentation.\n",
    "        \"\"\"\n",
    "        # Create a unique log file name with current date and time\n",
    "        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_file = self.log_dir / f\"feature_extraction_{self.mode.lower()}_{current_time}.log\"\n",
    "        \n",
    "        # Configure the logging system\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),      # Save messages to log file\n",
    "                logging.StreamHandler(sys.stdout)   # Also display on screen\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create our logger object\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Write initial log messages\n",
    "        self.logger.info(f\"=== Feature Extraction Pipeline Started ({self.mode}) ===\")\n",
    "        self.logger.info(f\"Base directory: {self.base_dir}\")\n",
    "        self.logger.info(f\"Device: {self.device}\")\n",
    "        self.logger.info(f\"Window size: {self.window_size_seconds}s ({self.window_size_samples} samples)\")\n",
    "        self.logger.info(f\"Expected windows per night: ~{self.expected_windows_per_night}\")\n",
    "        self.logger.info(f\"Batch size: {self.batch_size}\")\n",
    "    \n",
    "    def find_h5_files(self, directory):\n",
    "        \"\"\"\n",
    "        Look for all .h5 files in a specific directory.\n",
    "        These are the preprocessed files created by the preprocessing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            directory: The folder path where we want to search for .h5 files\n",
    "            \n",
    "        Returns:\n",
    "            A list of file paths for all .h5 files found in the directory\n",
    "        \"\"\"\n",
    "        # Search for .h5 files in the directory\n",
    "        h5_pattern = directory / \"*.h5\"\n",
    "        h5_files = glob.glob(str(h5_pattern))\n",
    "        \n",
    "        # Convert file paths from strings to Path objects\n",
    "        h5_files = [Path(f) for f in h5_files]\n",
    "        \n",
    "        # Sort files alphabetically for consistent processing order\n",
    "        h5_files.sort()\n",
    "        \n",
    "        # Log how many files we found\n",
    "        self.logger.info(f\"Found {len(h5_files)} .h5 files in {directory}\")\n",
    "        \n",
    "        return h5_files\n",
    "    \n",
    "    def load_participant_data(self, h5_file):\n",
    "        \"\"\"\n",
    "        Load preprocessed data from an .h5 file for one participant.\n",
    "        This reads all nights separately to preserve the night structure.\n",
    "        \n",
    "        IMPORTANT: Unlike the previous version, this function keeps nights SEPARATE\n",
    "        instead of concatenating them. This preserves the temporal structure needed\n",
    "        for machine learning models.\n",
    "        \n",
    "        Args:\n",
    "            h5_file: Path to the .h5 file to read\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing participant data organized by night\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract participant ID from filename (remove .h5 extension)\n",
    "            participant_id = h5_file.stem\n",
    "            \n",
    "            self.logger.info(f\"Loading participant: {participant_id}\")\n",
    "            \n",
    "            # Open the HDF5 file for reading\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                # Read participant information from file attributes\n",
    "                participant_name = f.attrs['name']\n",
    "                num_nights = f.attrs['number_of_nights']\n",
    "                \n",
    "                self.logger.info(f\"   - Participant: {participant_name}\")\n",
    "                self.logger.info(f\"   - Number of nights: {num_nights}\")\n",
    "                \n",
    "                # Initialize list to store data for each night SEPARATELY\n",
    "                nights_data = []  # Each element will be one night's data\n",
    "                \n",
    "                # Loop through all nights and collect each night's data separately\n",
    "                for night_num in range(1, num_nights + 1):\n",
    "                    night_group_name = f\"night{night_num}\"\n",
    "                    \n",
    "                    # Check if this night group exists in the file\n",
    "                    if night_group_name in f:\n",
    "                        night_group = f[night_group_name]\n",
    "                        \n",
    "                        # Read accelerometer data for this specific night\n",
    "                        x_data = night_group['x'][:]  # X-axis data\n",
    "                        y_data = night_group['y'][:]  # Y-axis data\n",
    "                        z_data = night_group['z'][:]  # Z-axis data\n",
    "                        \n",
    "                        # Read timestamps (stored as ISO format strings)\n",
    "                        timestamps_str = night_group['timestamps'][:]\n",
    "                        # Convert string timestamps back to datetime objects\n",
    "                        timestamps = [pd.Timestamp(ts.decode('utf-8')) for ts in timestamps_str]\n",
    "                        \n",
    "                        # Combine x, y, z data for this night\n",
    "                        night_data = np.column_stack([x_data, y_data, z_data])\n",
    "                        \n",
    "                        # Store this night's data as a separate entry\n",
    "                        nights_data.append({\n",
    "                            'night_number': night_num,\n",
    "                            'data': night_data,              # Shape: (samples_this_night, 3)\n",
    "                            'timestamps': timestamps,\n",
    "                            'samples': len(night_data),\n",
    "                            'duration_hours': len(night_data) / self.sampling_rate / 3600\n",
    "                        })\n",
    "                        \n",
    "                        self.logger.info(f\"     - Night {night_num}: {len(night_data):,} samples \"\n",
    "                                       f\"({len(night_data) / self.sampling_rate / 3600:.1f}h)\")\n",
    "                    else:\n",
    "                        self.logger.warning(f\"     - Night {night_num}: Group not found\")\n",
    "                \n",
    "                # Calculate total statistics\n",
    "                total_samples = sum(night['samples'] for night in nights_data)\n",
    "                total_duration_hours = sum(night['duration_hours'] for night in nights_data)\n",
    "                \n",
    "                self.logger.info(f\"   - Total samples across all nights: {total_samples:,}\")\n",
    "                self.logger.info(f\"   - Total duration: {total_duration_hours:.1f} hours\")\n",
    "                self.logger.info(f\"   - Valid nights loaded: {len(nights_data)}\")\n",
    "                \n",
    "                # Return all the participant information with nights kept separate\n",
    "                return {\n",
    "                    'participant_id': participant_id,\n",
    "                    'participant_name': participant_name,\n",
    "                    'num_nights': len(nights_data),  # Actual number of valid nights loaded\n",
    "                    'nights_data': nights_data,      # List of night data (NOT concatenated)\n",
    "                    'total_samples': total_samples,\n",
    "                    'total_duration_hours': total_duration_hours\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            # If anything goes wrong, log the error and re-raise it\n",
    "            self.logger.error(f\"Error loading {h5_file.name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_windows_for_night(self, night_data):\n",
    "        \"\"\"\n",
    "        Split one night's accelerometer data into fixed-size windows for feature extraction.\n",
    "        Each window contains 10 seconds of data (300 samples at 30Hz).\n",
    "        \n",
    "        This function processes ONE NIGHT at a time, preserving the night structure.\n",
    "        \n",
    "        Args:\n",
    "            night_data: Accelerometer data for one night (samples × 3)\n",
    "            \n",
    "        Returns:\n",
    "            numpy array: Windowed data for this night (num_windows × window_size × 3)\n",
    "        \"\"\"\n",
    "        total_samples = len(night_data)\n",
    "        \n",
    "        # Calculate how many complete windows we can create from this night\n",
    "        num_windows = total_samples // self.window_size_samples\n",
    "        \n",
    "        # Calculate how many samples we'll actually use (might be slightly less than total)\n",
    "        samples_used = num_windows * self.window_size_samples\n",
    "        \n",
    "        # ERROR CHECK: Make sure we have at least one complete window\n",
    "        if num_windows == 0:\n",
    "            raise ValueError(f\"Not enough data in this night to create even one window. \"\n",
    "                           f\"Need at least {self.window_size_samples} samples, got {total_samples}.\")\n",
    "        \n",
    "        # Reshape the data into windows\n",
    "        # Take only the samples that fit into complete windows\n",
    "        data_for_windowing = night_data[:samples_used]\n",
    "        \n",
    "        # Reshape into windows: (num_windows, window_size_samples, 3)\n",
    "        windows = data_for_windowing.reshape(num_windows, self.window_size_samples, 3)\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def extract_features_batch(self, windows_batch):\n",
    "        \"\"\"\n",
    "        Extract SSL-Wearables features from a batch of windows.\n",
    "        This is where the actual feature extraction happens using the pre-trained model.\n",
    "        \n",
    "        Args:\n",
    "            windows_batch: Batch of windows (batch_size × window_size × 3)\n",
    "            \n",
    "        Returns:\n",
    "            numpy array: Feature vectors (batch_size × 1024)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert numpy array to PyTorch tensor\n",
    "            # SSL-Wearables expects input in format: (batch, channels, samples)\n",
    "            # Our windows are in format: (batch, samples, channels)\n",
    "            # So we need to swap the last two dimensions\n",
    "            windows_tensor = torch.tensor(windows_batch, dtype=torch.float32).permute(0, 2, 1)\n",
    "            \n",
    "            # Move tensor to GPU (if available) for faster processing\n",
    "            windows_tensor = windows_tensor.to(self.device)\n",
    "            \n",
    "            # Extract features using SSL-Wearables feature extractor\n",
    "            with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "                features = self.ssl_model.feature_extractor(windows_tensor).squeeze(-1)\n",
    "                feature_batch = features.detach().cpu().numpy()\n",
    "\n",
    "            return feature_batch\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in feature extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_features_for_night(self, night_data):\n",
    "        \"\"\"\n",
    "        Extract SSL-Wearables features for one complete night.\n",
    "        This processes all windows from one night and returns the features.\n",
    "        \n",
    "        Args:\n",
    "            night_data: Dictionary containing one night's accelerometer data\n",
    "            \n",
    "        Returns:\n",
    "            numpy array: Feature vectors for this night (num_windows × 1024)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            night_num = night_data['night_number']\n",
    "            data = night_data['data']\n",
    "            \n",
    "            self.logger.info(f\"   Extracting features for night {night_num}\")\n",
    "            \n",
    "            # STEP 1: Create windows from this night's data\n",
    "            windows = self.create_windows_for_night(data)\n",
    "            num_windows = len(windows)\n",
    "            \n",
    "            self.logger.info(f\"       - Windows created: {num_windows:,} \"\n",
    "                           f\"(expected ~{self.expected_windows_per_night})\")\n",
    "            \n",
    "            # STEP 2: Process windows in batches for efficiency\n",
    "            all_features = []  # List to store feature vectors from all batches\n",
    "            \n",
    "            # Calculate how many batches we need\n",
    "            num_batches = (num_windows + self.batch_size - 1) // self.batch_size  # Ceiling division\n",
    "            \n",
    "            # Process each batch\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Calculate start and end indices for this batch\n",
    "                start_idx = batch_idx * self.batch_size\n",
    "                end_idx = min(start_idx + self.batch_size, num_windows)\n",
    "                \n",
    "                # Extract windows for this batch\n",
    "                batch_windows = windows[start_idx:end_idx]\n",
    "                \n",
    "                # Extract features for this batch\n",
    "                batch_features = self.extract_features_batch(batch_windows)\n",
    "                \n",
    "                # Add to our collection of all features\n",
    "                all_features.append(batch_features)\n",
    "                \n",
    "                # Clear GPU cache to prevent memory buildup\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # STEP 3: Combine all batch results for this night\n",
    "            night_features = np.concatenate(all_features, axis=0)\n",
    "            \n",
    "            self.logger.info(f\"       - Features extracted: {night_features.shape}\")\n",
    "            \n",
    "            return night_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting features for night {night_num}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_participant_features(self, participant_data):\n",
    "        \"\"\"\n",
    "        Extract SSL-Wearables features for one complete participant.\n",
    "        This processes all nights for one participant and organizes features by night.\n",
    "        \n",
    "        IMPORTANT: This function maintains the night structure:\n",
    "        Output shape: (nights, windows_per_night, 1024_features)\n",
    "        \n",
    "        Args:\n",
    "            participant_data: Dictionary containing participant's accelerometer data organized by night\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing extracted features organized by night\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id = participant_data['participant_id']\n",
    "            nights_data = participant_data['nights_data']\n",
    "            \n",
    "            # Record start time for performance tracking\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            self.logger.info(f\"Extracting features for {participant_id} ({len(nights_data)} nights)\")\n",
    "            \n",
    "            # Initialize list to store features for each night\n",
    "            all_nights_features = []  # Each element will be features for one night\n",
    "            total_windows = 0\n",
    "            \n",
    "            # Process each night separately\n",
    "            for night_data in nights_data:\n",
    "                # Extract features for this specific night\n",
    "                night_features = self.extract_features_for_night(night_data)\n",
    "                \n",
    "                # Add this night's features to our collection\n",
    "                all_nights_features.append(night_features)\n",
    "                total_windows += len(night_features)\n",
    "            \n",
    "            # Convert list of night features to a structured numpy array\n",
    "            # We need to handle the fact that different nights might have different numbers of windows\n",
    "            \n",
    "            # Find the maximum number of windows across all nights\n",
    "            max_windows_per_night = max(len(night_features) for night_features in all_nights_features)\n",
    "            \n",
    "            # Create a padded array to store all nights' features\n",
    "            # Shape: (num_nights, max_windows_per_night, 1024)\n",
    "            # We'll pad shorter nights with zeros\n",
    "            num_nights = len(all_nights_features)\n",
    "            participant_features = np.zeros((num_nights, max_windows_per_night, self.feature_dim))\n",
    "            \n",
    "            # Fill in the actual features for each night\n",
    "            for night_idx, night_features in enumerate(all_nights_features):\n",
    "                num_windows_this_night = len(night_features)\n",
    "                participant_features[night_idx, :num_windows_this_night, :] = night_features\n",
    "            \n",
    "            # Also create a mask to indicate which windows are real vs padded\n",
    "            windows_mask = np.zeros((num_nights, max_windows_per_night), dtype=bool)\n",
    "            for night_idx, night_features in enumerate(all_nights_features):\n",
    "                num_windows_this_night = len(night_features)\n",
    "                windows_mask[night_idx, :num_windows_this_night] = True\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Log final results\n",
    "            self.logger.info(f\" Feature extraction completed:\")\n",
    "            self.logger.info(f\"   - Nights processed: {num_nights}\")\n",
    "            self.logger.info(f\"   - Total windows: {total_windows:,}\")\n",
    "            self.logger.info(f\"   - Features shape: {participant_features.shape}\")\n",
    "            self.logger.info(f\"   - Max windows per night: {max_windows_per_night}\")\n",
    "            self.logger.info(f\"   - Processing time: {processing_time:.1f} seconds\")\n",
    "            self.logger.info(f\"   - Speed: {total_windows/processing_time:.1f} windows/second\")\n",
    "            \n",
    "            # Return all the results with night structure preserved\n",
    "            return {\n",
    "                'participant_id': participant_id,\n",
    "                'features': participant_features,       # Shape: (nights, max_windows_per_night, 1024)\n",
    "                'windows_mask': windows_mask,           # Shape: (nights, max_windows_per_night) - True for real windows\n",
    "                'num_nights': num_nights,\n",
    "                'total_windows': total_windows,\n",
    "                'max_windows_per_night': max_windows_per_night,\n",
    "                'windows_per_night': [len(night_features) for night_features in all_nights_features],\n",
    "                'processing_time': processing_time,\n",
    "                'windows_per_second': total_windows / processing_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting features for {participant_id}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_participant_features(self, feature_data, output_path):\n",
    "        \"\"\"\n",
    "        Save extracted features for one participant to a .npy file.\n",
    "        The features are saved with night structure preserved.\n",
    "        \n",
    "        Args:\n",
    "            feature_data: Dictionary containing participant features and metadata\n",
    "            output_path: Full path where to save the feature file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id = feature_data['participant_id']\n",
    "            features = feature_data['features']\n",
    "            windows_mask = feature_data['windows_mask']\n",
    "            \n",
    "            # Create a dictionary with all the data we want to save\n",
    "            save_data = {\n",
    "                'features': features,                    # Shape: (nights, max_windows_per_night, 1024)\n",
    "                'windows_mask': windows_mask,            # Shape: (nights, max_windows_per_night)\n",
    "                'participant_id': participant_id,\n",
    "                'num_nights': feature_data['num_nights'],\n",
    "                'total_windows': feature_data['total_windows'],\n",
    "                'max_windows_per_night': feature_data['max_windows_per_night'],\n",
    "                'windows_per_night': feature_data['windows_per_night'],\n",
    "                'extraction_date': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Save as numpy file (can store dictionaries)\n",
    "            np.save(output_path, save_data)\n",
    "            \n",
    "            # Log successful save\n",
    "            self.logger.info(f\" Saved features: {output_path.name}\")\n",
    "            self.logger.info(f\"   - Shape: {features.shape}\")\n",
    "            self.logger.info(f\"   - File size: {output_path.stat().st_size / 1e6:.1f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving features for {feature_data['participant_id']}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def process_participant(self, h5_file, group_type):\n",
    "        \"\"\"\n",
    "        Process one participant from start to finish.\n",
    "        This is the main processing function that orchestrates all steps for one person.\n",
    "        \n",
    "        Args:\n",
    "            h5_file: Path to the participant's .h5 file\n",
    "            group_type: Either 'controls' or 'irbd'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract participant ID from filename\n",
    "            participant_id = h5_file.stem\n",
    "            \n",
    "            # Log that we're starting to process this participant\n",
    "            self.logger.info(f\"\\n{'='*60}\")\n",
    "            self.logger.info(f\"Processing {group_type.upper()}: {participant_id}\")\n",
    "            self.logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # STEP 1: Load participant data from .h5 file (keeping nights separate)\n",
    "            participant_data = self.load_participant_data(h5_file)\n",
    "            \n",
    "            # STEP 2: Extract SSL-Wearables features (preserving night structure)\n",
    "            feature_data = self.extract_participant_features(participant_data)\n",
    "            \n",
    "            # STEP 3: Save features to appropriate location\n",
    "            if self.mode == \"EXAMPLE_TEST\":\n",
    "                # For example testing, save to root of features directory\n",
    "                output_path = self.features_dir / \"example_file_features.npy\"\n",
    "            else:\n",
    "                # For full processing, save to appropriate group directory\n",
    "                if group_type == 'controls':\n",
    "                    output_dir = self.features_controls_dir\n",
    "                else:\n",
    "                    output_dir = self.features_irbd_dir\n",
    "                output_path = output_dir / f\"{participant_id}_features.npy\"\n",
    "            \n",
    "            self.save_participant_features(feature_data, output_path)\n",
    "            \n",
    "            # STEP 4: Update statistics\n",
    "            self.stats['processed_participants'] += 1\n",
    "            self.stats['total_nights'] += feature_data['num_nights']\n",
    "            self.stats['total_windows'] += feature_data['total_windows']\n",
    "            self.stats['total_features_extracted'] += feature_data['total_windows']\n",
    "            self.stats['processing_time_total'] += feature_data['processing_time']\n",
    "            \n",
    "            if group_type == 'controls':\n",
    "                self.stats['controls_processed'] += 1\n",
    "            else:\n",
    "                self.stats['irbd_processed'] += 1\n",
    "            \n",
    "            # Log success\n",
    "            self.logger.info(f\"{participant_id} processed successfully:\")\n",
    "            self.logger.info(f\"   - {feature_data['num_nights']} nights\")\n",
    "            self.logger.info(f\"   - {feature_data['total_windows']:,} windows\")\n",
    "            self.logger.info(f\"   - Features shape: {feature_data['features'].shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If anything goes wrong, log the error but continue with other participants\n",
    "            self.logger.error(f\"Error processing {participant_id}: {str(e)}\")\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            self.stats['failed_participants'] += 1\n",
    "    \n",
    "    def run_example_test(self):\n",
    "        \"\"\"\n",
    "        Run feature extraction on just one example participant for testing.\n",
    "        This saves the result as 'example_file_features.npy' in the root features directory.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"EXAMPLE TEST MODE: Processing example participant\")\n",
    "        \n",
    "        # Look for the example participant in both directories\n",
    "        example_file = None\n",
    "        group_type = None\n",
    "        \n",
    "        # Check controls directory first\n",
    "        controls_file = self.preprocessed_controls_dir / f\"{self.example_participant}.h5\"\n",
    "        if controls_file.exists():\n",
    "            example_file = controls_file\n",
    "            group_type = 'controls'\n",
    "        else:\n",
    "            # Check iRBD directory\n",
    "            irbd_file = self.preprocessed_irbd_dir / f\"{self.example_participant}.h5\"\n",
    "            if irbd_file.exists():\n",
    "                example_file = irbd_file\n",
    "                group_type = 'irbd'\n",
    "        \n",
    "        # Check if we found the example file\n",
    "        if example_file is None:\n",
    "            self.logger.error(f\"Example participant not found: {self.example_participant}\")\n",
    "            self.logger.error(f\"Checked: {controls_file}\")\n",
    "            self.logger.error(f\"Checked: {irbd_file}\")\n",
    "            return\n",
    "        \n",
    "        # Set statistics for processing one participant\n",
    "        self.stats['total_participants'] = 1\n",
    "        \n",
    "        # Process the example participant\n",
    "        self.process_participant(example_file, group_type)\n",
    "        \n",
    "        # Show final results\n",
    "        self.print_final_statistics()\n",
    "        \n",
    "        # Report success or failure\n",
    "        if self.stats['processed_participants'] > 0:\n",
    "            self.logger.info(\"EXAMPLE TEST SUCCESSFUL!\")\n",
    "            self.logger.info(f\"Features saved to: {self.features_dir}/example_file_features.npy\")\n",
    "            self.logger.info(\"Ready for full dataset processing!\")\n",
    "        else:\n",
    "            self.logger.error(\"EXAMPLE TEST FAILED!\")\n",
    "    \n",
    "    def run_full_processing(self):\n",
    "        \"\"\"\n",
    "        Run feature extraction on all participants in the dataset.\n",
    "        This processes all .h5 files in both controls and iRBD directories.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"FULL PROCESSING MODE: Processing all participants\")\n",
    "        \n",
    "        # Find all .h5 files in both directories\n",
    "        controls_files = self.find_h5_files(self.preprocessed_controls_dir)\n",
    "        irbd_files = self.find_h5_files(self.preprocessed_irbd_dir)\n",
    "        \n",
    "        # Calculate total number of participants\n",
    "        total_participants = len(controls_files) + len(irbd_files)\n",
    "        self.stats['total_participants'] = total_participants\n",
    "        \n",
    "        # Check if we found any files\n",
    "        if total_participants == 0:\n",
    "            self.logger.error(\"No .h5 files found in input directories\")\n",
    "            return\n",
    "        \n",
    "        # Log what we found\n",
    "        self.logger.info(f\"Found {total_participants} participants:\")\n",
    "        self.logger.info(f\"   - Controls: {len(controls_files)} participants\")\n",
    "        self.logger.info(f\"   - iRBD: {len(irbd_files)} participants\")\n",
    "        \n",
    "        # Process all control participants\n",
    "        self.logger.info(f\"\\n Processing CONTROLS ({len(controls_files)} participants)...\")\n",
    "        for i, h5_file in enumerate(controls_files, 1):\n",
    "            self.logger.info(f\"\\n--- Controls Progress: {i}/{len(controls_files)} ---\")\n",
    "    \n",
    "            # CHECK IF ALREADY PROCESSED (RESUME FUNCTIONALITY)\n",
    "            participant_id = h5_file.stem\n",
    "            feature_file = self.features_controls_dir / f\"{participant_id}_features.npy\"\n",
    "            if feature_file.exists():\n",
    "                self.logger.info(f\"✓ Skipping {participant_id} - features already exist\")\n",
    "                self.stats['processed_participants'] += 1\n",
    "                self.stats['controls_processed'] += 1\n",
    "                continue\n",
    "    \n",
    "            self.process_participant(h5_file, 'controls')\n",
    "\n",
    "        # Process all iRBD participants\n",
    "        self.logger.info(f\"\\n Processing iRBD ({len(irbd_files)} participants)...\")\n",
    "        for i, h5_file in enumerate(irbd_files, 1):\n",
    "            self.logger.info(f\"\\n--- iRBD Progress: {i}/{len(irbd_files)} ---\")\n",
    "    \n",
    "            # CHECK IF ALREADY PROCESSED (RESUME FUNCTIONALITY)\n",
    "            participant_id = h5_file.stem\n",
    "            feature_file = self.features_irbd_dir / f\"{participant_id}_features.npy\"\n",
    "            if feature_file.exists():\n",
    "                self.logger.info(f\"✓ Skipping {participant_id} - features already exist\")\n",
    "                self.stats['processed_participants'] += 1\n",
    "                self.stats['irbd_processed'] += 1\n",
    "                continue\n",
    "    \n",
    "            self.process_participant(h5_file, 'irbd')\n",
    "        \n",
    "        # Show final statistics\n",
    "        self.print_final_statistics()\n",
    "    \n",
    "    def run_feature_extraction(self):\n",
    "        \"\"\"\n",
    "        Main function to run the feature extraction pipeline.\n",
    "        Decides whether to run example test or full processing based on configuration.\n",
    "        \"\"\"\n",
    "        if self.mode == \"EXAMPLE_TEST\":\n",
    "            self.run_example_test()\n",
    "        else:\n",
    "            self.run_full_processing()\n",
    "    \n",
    "    def print_final_statistics(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive summary of the feature extraction results.\n",
    "        Shows processing success rates, performance metrics, and dataset statistics.\n",
    "        \"\"\"\n",
    "        # Print header\n",
    "        self.logger.info(f\"\\n{'='*60}\")\n",
    "        self.logger.info(f\"FEATURE EXTRACTION COMPLETED ({self.mode})\")\n",
    "        self.logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # Participant processing statistics\n",
    "        self.logger.info(f\"Participant Statistics:\")\n",
    "        self.logger.info(f\"  - Total participants: {self.stats['total_participants']}\")\n",
    "        self.logger.info(f\"  - Successfully processed: {self.stats['processed_participants']}\")\n",
    "        self.logger.info(f\"  - Failed to process: {self.stats['failed_participants']}\")\n",
    "        \n",
    "        # Calculate success rate\n",
    "        if self.stats['total_participants'] > 0:\n",
    "            success_rate = self.stats['processed_participants'] / self.stats['total_participants'] * 100\n",
    "            self.logger.info(f\"  - Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Group breakdown\n",
    "        self.logger.info(f\"Group Breakdown:\")\n",
    "        self.logger.info(f\"  - Controls processed: {self.stats['controls_processed']}\")\n",
    "        self.logger.info(f\"  - iRBD processed: {self.stats['irbd_processed']}\")\n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Data processing statistics\n",
    "        self.logger.info(f\"Data Processing:\")\n",
    "        self.logger.info(f\"  - Total nights: {self.stats['total_nights']}\")\n",
    "        self.logger.info(f\"  - Total windows: {self.stats['total_windows']:,}\")\n",
    "        self.logger.info(f\"  - Total features extracted: {self.stats['total_features_extracted']:,}\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        if self.stats['processed_participants'] > 0:\n",
    "            avg_nights = self.stats['total_nights'] / self.stats['processed_participants']\n",
    "            avg_windows = self.stats['total_windows'] / self.stats['processed_participants']\n",
    "            self.logger.info(f\"  - Average nights per participant: {avg_nights:.1f}\")\n",
    "            self.logger.info(f\"  - Average windows per participant: {avg_windows:.0f}\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Performance statistics\n",
    "        self.logger.info(f\"Performance:\")\n",
    "        self.logger.info(f\"  - Total processing time: {self.stats['processing_time_total']:.1f} seconds\")\n",
    "        \n",
    "        if self.stats['processing_time_total'] > 0:\n",
    "            windows_per_second = self.stats['total_windows'] / self.stats['processing_time_total']\n",
    "            self.logger.info(f\"  - Overall speed: {windows_per_second:.1f} windows/second\")\n",
    "        \n",
    "        self.logger.info(\"\")\n",
    "        \n",
    "        # Final success message\n",
    "        if self.stats['processed_participants'] > 0:\n",
    "            self.logger.info(\"Feature extraction pipeline completed successfully!\")\n",
    "            if self.mode == \"EXAMPLE_TEST\":\n",
    "                self.logger.info(f\"Example features saved to: /data/features/example_file_features.npy\")\n",
    "        else:\n",
    "            self.logger.error(\"Feature extraction pipeline failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that runs when the script is executed directly.\n",
    "    Creates a FeatureExtractionPipeline object and runs the entire process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create and run the feature extraction pipeline\n",
    "        pipeline = FeatureExtractionPipeline()\n",
    "        pipeline.run_feature_extraction()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Feature extraction interrupted by user\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Feature extraction failed with error: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# SCRIPT EXECUTION ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR THE EXAMPLE FILE\n",
    "# 7 nights processed\n",
    "# 864000 samples per night (8hours at 30 Hz)\n",
    "# 48 segments per night (10-minute segments)\n",
    "# 60 windows per segment (10-second windows)\n",
    "# 2880 windows per night total\n",
    "\n",
    "# Each segment: (60, 1024) features\n",
    "# Each night: (2880, 1024) features\n",
    "# Total: 7 x 2880 = 20160 windows with 1024D features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Feature Extraction)",
   "language": "python",
   "name": "env_feature_extraction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
